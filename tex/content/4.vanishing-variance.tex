\section{The limit for fixed \texorpdfstring{\( \lambda \)}{lambda}}

\red{TODO: text}

\subsection{Vanishing variance of the empirical martingale}
The empirical martingale from \zcref{subsec:emp-mart} will be our workhorse to establish relative compactness.
This will be possible because it satisfies a key property, which we will prove in this section.
Namely, we will show that the variance of \((\ol{M}_t^{\lambda,N}[\psi])_{t\in[0,T]}\) vanishes for large \(N\), implying that it has a deterministic limit.    % TODO does that imply that? Maybe Klebaner Corollary 7.30 is relevant (although our martingale is not continuous, and we are talking about the quadratic variation, so probably not directly)

Define
\begin{equation}
  S_s^N[\psi] \coloneqq \int_\potspace \psi \odif{\mu_s^N},
  \quad
  A_s^{\lambda, N}[\psi] \coloneqq \int_0^s \int_\potspace \ol{L}_{\mu_r^N}^{\lambda} [\psi] \odif{\mu_{r}^N} \odif{r}, \quad s \in [0,T].
\end{equation}
Then we have
\begin{equation}\label{eq:M=S-A}
  \ol{M}_t^{\lambda,N}[\psi] = S_t^N[\psi] - S_0^N[\psi] - A_t^{\lambda,N}[\psi].
\end{equation}
Note that \((A_s^{\lambda, N}[\psi])_{s\in[0,T]}\) is a process of finite variation, because it is absolutely continuous.
(This absolute continuity follows directly from the defining expression of \((A_s^N)\).) % which is of "fundamental theorem of calculus"-form.
Thus, \((S_s^N[\psi])_{s\in[0,T]}\) is the sum of a martingale and a process of finite variation, making it a semimartingale by \zcref{def:semimart}.

\begin{proposition}\label{prop:vanishing-variance}
  For all \(\psi \in C_c^1(\potspace)\), \(\lambda > 0\), \(t \in [0,T]\),
  \begin{equation}
    \forall N \in \bN : \Var{\lawmu^{\lambda,N}}{\ol{M}_t^{\lambda,N}[\psi]} = \Exp{\lawmu^{\lambda,N}}{(\ol{M}_t^N)^2} = \Exp{\lawmu^{\lambda,N}}{\sbr{\ol{M}^N}_t},
  \end{equation}
  and
  \begin{equation}
    \lim_{N\to\infty} \Var{\lawmu^{\lambda,N}}{\ol{M}_t^{\lambda,N}[\psi]} = 0.
  \end{equation}
\end{proposition}
\begin{proof}
  Throughout this proof we suppress the dependence of \( \ol{M} \), \( S \) and \( A \) on \( \psi \) and \( \lambda \), as both are fixed.
  We will vary \( N \in \bN \), but fix it for now.
  Recall that \((\ol{M}_s^N)_{s\in[0,T]}\) is a martingale by \zcref{prop:emp-M-mart}.
  In particular,
  \begin{equation}
    \Exp{\lawmu^{\lambda,N}}{\ol{M}_t^N} = \Exp{\lawmu^{\lambda,N}}{\ol{M}_0^N}
    = \Exp{\lawmu^{\lambda,N}}{0} = 0.    % the second-to-last step is obvious from \zcref{eq:M-def}
  \end{equation}
  Due to \zcref{lem:emp-mart-bounded} \( \ol{M}_t^N \) is bounded, so in particular square integrable.
  Hence
  \begin{equation}
    \Var{\lawmu^{\lambda,N}}{\ol{M}_t^{\lambda,N}[\psi]} = \Exp{\lawmu^{\lambda,N}}{(\ol{M}_t^N)^2}.
  \end{equation}

  % TODO: what was the idea here? Mentioning the predictability of M here already (or further above) might be good though
  % \begin{equation}
  %     (\ol{M}_t^N)^2 = \abr{\ol{M}^N}_t + 2 \int_0^t \ol{M}_{s-}^N \odif{\ol{M}_s^N}
  % \end{equation}
  % Note that \((\ol{M}_s^N)_{s\in[0,T]}\) is càdlàg by \zcref{eq:M-def} and adapted, so \((\ol{M}_{s-}^N)_{s\in[0,T]}\) is left-continuous and adapted.
  % This means that it is predictable (see e.g. Definition 8.3 \cite{klebanerIntroductionStochasticCalculus2012}), so as \((\ol{M}_s^N)_{s\in[0,T]}\) is bounded the stochastic integral is well-defined.

  We can relate the second moment to the expectation of the quadratic variation using Theorem 7.27 from~\cite[201]{klebanerIntroductionStochasticCalculus2012}.
  Namely, since \(\ol{M}_0^N = 0\),
  \begin{equation}
    \Exp{\lawmu^{\lambda,N}}{(\ol{M}_t^N)^2} = \Exp{\lawmu^{\lambda,N}}{\sbr{\ol{M}^N}_t}.
  \end{equation}
  This establishes the first claim we had to show.

  By Theorem 8.6 in~\cite[221]{klebanerIntroductionStochasticCalculus2012} we have
  \begin{equation}\label{eq:S^2-q-var-rel}
    \begin{split}
      \sbr{S^N}_t & = \sbr{S^N - S_0^N}_t                                                                                \\   % this is clear from the definition of covariation: constants don't vary
                  & = (S_t^N - S_0^N)^2 - 2 \int_0^t (S_{r-}^N - S_0^N) \odif{\br{S_r^N - S_0^N}}                        \\
                  & = (S_t^N)^2 - 2 S_t^N S_0^N + (S_0^N)^2 - 2 \int_0^t (S_{r-}^N - S_0^N) \odif{S_r^N}                 \\   % again, constants don't vary so the don't matter when integrating against them
                  & = (S_t^N)^2 - 2 S_t^N S_0^N + (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{S_r^N} + 2 S_0^N (S_t^N - S_0^N) \\
                  & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{S_r^N}.
    \end{split}
  \end{equation}
  Now we are going to reuse the original martingale from \zcref{thm:mart-problem-sol}, but we will choose \(\varphi \coloneqq \br{ \frac{1}{N} \bigoplus_{i=1}^N \psi }^2\) as the test function this time.
  Recall \zcref{eq:M-def} and \zcref{eq:mart-problem-s-}, which together yield
  \begin{equation}
    M_t^{\lambda, N}[\varphi]
    = \varphi(\bm{X}_t) - \varphi(\bm{X}_0) - \int_0^t L^{\lambda, N} \varphi (\bm{X}_{r-}) \odif{r}
    = \varphi(\bm{X}_t) - \varphi(\bm{X}_0) - \int_0^t (L_{\cD}^{N} + L_{\cJ}^{\lambda, N}) \varphi (\bm{X}_{r-}) \odif{r}.
  \end{equation}
  We will substiture the aforementioned choice of \( \varphi \) into this expression term by term, starting with
  \begin{equation}
    \begin{split}
      \varphi(\bm{X}_t) - \varphi(\bm{X}_0)
       & = \br*{\frac{1}{N} \sum_{i=1}^N \psi(X_t^i)}^2 - \br*{\frac{1}{N} \sum_{i=1}^N \psi(X_0^i)}^2 \\
       & = \br*{ \int_\potspace \psi \odif{\mu_t^N} }^2 - \br*{ \int_\potspace \psi \odif{\mu_0^N} }^2
      = (S_t^N)^2 - (S_0^N)^2.
    \end{split}
  \end{equation}

  Next, we have
  \begin{equation}
    \begin{split}
      (L_{\cD}^{N} \varphi) (\bm{X}_{r-})
       & = \sum_{i=1}^N b(X_{r-}^i) \partial_{x^i}\br*{ \frac{1}{N} \bigoplus_{j=1}^N \psi }^2 (\bm{X}_{r-})                  \\
       & = \frac{1}{N^2} \sum_{i=1}^N b(X_{r-}^i) \cdot 2 \br*{ \sum_{j=1}^N \psi(X_{r-}^j) } \cdot \partial_x \psi(X_{r-}^i) \\
       & = \frac{2}{N^2} \sum_{j=1}^N \psi(X_{r-}^j) \sum_{i=1}^N b(X_{r-}^i) \partial_x \psi(X_{r-}^i)                       \\
       & = 2 \int_\potspace \psi \odif{\mu_{r-}^N} \int_\potspace b(x) \partial_x \psi (x) \mu_{r-}^N(\odif{x})               \\
       & = 2 S_{r-}^N \int_\potspace b(x) \partial_x \psi (x) \mu_{r-}^N(\odif{x}).
    \end{split}
  \end{equation}

  Finally, similar to \zcref{eq:kappa-to-mean-kappa},
  \begin{equation}
    \begin{split}
      (L_{\cJ}^{\lambda, N} \varphi) (\bm{X}_{r-})
       & = \int_{\potspace^N} \sbr*{ \br*{ \frac{1}{N} \sum_{j=1}^N \psi(y^j) }^2 - \br*{ \frac{1}{N} \sum_{j=1}^N \psi(X_{r-}^j) }^2 } \lambda \kappa(\bm{X}_{r-},\odif{\bm{y}})                                                                      \\
       & = \sum_{i=1}^N \int_{\potspace^N} \sbr*{ \br*{ \frac{1}{N} \sum_{j=1}^N \psi(y^j) }^2 - \br*{ \frac{1}{N} \sum_{j=1}^N \psi(X_{r-}^j) }^2 } \lambda \delta_{\jtarg^i(\bm{X}_{r-})}(\odif{y^i}) \prod_{k \neq i} \delta_{X_{r-}^k}(\odif{y^k}) \\
      %
       & = \frac{\lambda}{N^2} \sum_{i=1}^N \sbr*{ \br*{  \sum_{\substack{j=1                                                                                                                                                                          \\ j \neq i}}^N \psi(X_{r-}^j) + \psi(\jtarg^i(\bm{X}_{r-})) }^2 - \br*{ \sum_{\substack{j=1 \\ j \neq i}}^N \psi(X_{r-}^j) + \psi(X_{r-}^i) }^2 }  \\
      %
      %  & = \frac{\lambda}{N^2} \sum_{i=1}^N \sbr*{ \br*{ \sum_{\substack{j=1 \\ j \neq i}}^N \psi(X_{r-}^j) }^2 + 2 \psi(\jtarg^i(\bm{X}_{r-})) \sum_{\substack{j=1 \\ j \neq i}}^N \psi(X_{r-}^i) + \psi^2(\jtarg^i(\bm{X}_{r-})) - \br*{ \sum_{\substack{j=1 \\ j \neq i}}^N \psi(X_{r-}^j) + \psi(X_{r-}^i) }^2 }^2 } \\
      %
      % Simply using (a + b)^2 - (a + c)^2 = a^2 + 2 a b + b^2 - ( a^2 + 2 a c + c^2 ) = 2 (b - c) a + b^2 - c^2
       & = \frac{\lambda}{N^2} \sum_{i=1}^N \sbr*{ 2 (\psi(\jtarg^i(\bm{X}_{r-})) - \psi(X_{r-}^i)) \br*{\sum_{\substack{j=1                                                                                                                           \\ j \neq i}}^N \psi(X_{r-}^j)} + \psi^2(\jtarg^i(\bm{X}_{r-})) - \psi^2(X_{r-}^i) } \\
      % Using the identity \jtarg^i(\bm{X}_{r-}) = \ol{\jtarg}_{\mu_{r-}^N}(X_{r-}^i), which holds due to the uniform, scaled weight choice
       & = \frac{\lambda}{N^2} \sum_{i=1}^N \sbr*{ 2 (\psi(\ol{\jtarg}_{\mu_{r-}^N}(X_{r-}^i)) - \psi(X_{r-}^i)) \br*{\sum_{j=1}^N \psi(X_{r-}^j) - \psi(X_{r-}^i)} + \psi^2(\ol{\jtarg}_{\mu_{r-}^N}(X_{r-}^i)) - \psi^2(X_{r-}^i) }                  \\
      %
       & = \frac{1}{N^2} \sum_{i=1}^N \int_\potspace \sbr*{ 2 (\psi(y) - \psi(X_{r-}^i)) \br*{\sum_{j=1}^N \psi(X_{r-}^j) - \psi(X_{r-}^i)} + \psi^2(y) - \psi^2(X_{r-}^i) } \lambda \ol{\kappa}_{\mu_{r-}^N}(X_{r-}^i, \odif{y})                      \\
      %
       & = \frac{1}{N} \int_\potspace \int_\potspace \sbr*{ 2 (\psi(y) - \psi(x)) \br*{\sum_{j=1}^N \psi(X_{r-}^j)} - 2 (\psi(y) - \psi(x)) \psi(x) + \psi^2(y) - \psi^2(x) } \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) \mu_{r-}^N(\odif{x}).
    \end{split}
  \end{equation}
  Recall that \( \psi \) is bounded, so that in particular \( (x,y) \mapsto - 2 (\psi(y) - \psi(x)) \psi(x) + \psi^2(y) - \psi^2(x) \) is bounded, and that \( \ol{\kappa}_{\mu_{r-}^N} \) and \( \mu_{r-}^N \) are both finite (probability) measures.
  This implies that
  \begin{equation}
    \begin{split}
      (L_{\cJ}^{\lambda, N} \varphi) (\bm{X}_{r-})
       & = \frac{2}{N} \br*{\sum_{j=1}^N \psi(X_{r-}^j)}  \int_\potspace \int_\potspace (\psi(y) - \psi(x)) \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) \mu_{r-}^N(\odif{x}) + \bigO{N^{-1}} \\
       & = 2 \int_\potspace \psi \odif{\mu_{r-}^N} \int_\potspace \int_\potspace (\psi(y) - \psi(x)) \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) \mu_{r-}^N(\odif{x})  + \bigO{N^{-1}}       \\
       & = 2 S_{r-}^N \int_\potspace \int_\potspace (\psi(y) - \psi(x)) \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) \mu_{r-}^N(\odif{x})  + \bigO{N^{-1}},
    \end{split}
  \end{equation}
  for \( N \to \infty \).
  Combining the terms now yields
  \begin{equation}
    \begin{split}
      M_t^{\lambda, N}[\varphi]
                                         & = \varphi(\bm{X}_t) - \varphi(\bm{X}_0) - \int_0^t (L_{\cD}^{N} + L_{\cJ}^{\lambda, N}) \varphi (\bm{X}{r-}) \odif{r}                                                                                                                                                          \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t \cbr*{ S_{r-}^N \int_\potspace b(x) \partial_x \psi (x) \mu_{r-}^N(\odif{x}) + S_{r-}^N \int_\potspace \int_\potspace (\psi(y) - \psi(x)) \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) \mu_{r-}^N(\odif{x})  + \bigO{N^{-1}} }  \odif{r} \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \int_\potspace \cbr*{ b(x) \partial_x \psi (x) + \int_\potspace (\psi(y) - \psi(x)) \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) } \mu_{r-}^N(\odif{x})  \odif{r} + \bigO{N^{-1}}                                               \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \int_\potspace \ol{L}_{\mu_{r-}^N}^{\lambda} [\psi] \odif{\mu_{r-}^N} \odif{r} + \bigO{N^{-1}}                                                                                                                                   \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odv{A_{r-}^N}{r} \odif{r} + \bigO{N^{-1}}                                                                                                                                                                                       \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{A_{r-}^N} + \bigO{N^{-1}}                                                                                                                                                                                                  \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{A_r^N} + \bigO{N^{-1}}                                                                                                                                                                                                     \\   % A^N is (absolutely) continuous
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{\br*{S_r^N - S_0^N - \ol{M}_r^N}} + \mathcal{O}(N^{-1})                                                                                                                                                                    \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{S_r^N} + 2 \int_0^t S_{r-}^N \odif{\ol{M}_r^N} + \mathcal{O}(N^{-1})                                                                                                                                                       \\
      \overset{\eqref{eq:S^2-q-var-rel}} & {=} \sbr{S^N}_t + 2 \int_0^t S_{r-}^N \odif{\ol{M}_r^N} + \mathcal{O}(N^{-1}).
    \end{split}
  \end{equation}

  By the polarization identity for quadratic variation, we have
  \begin{equation}
    \sbr{S^N}_t
    = \sbr{S^N - S_0^N}_t
    = \sbr{\ol{M}^N + A^N}_t
    = \sbr{\ol{M}^N}_t + 2 \sbr{ \ol{M}^N, A^N }_t + \sbr{ A^N }_t
    = \sbr{\ol{M}^N}_t
  \end{equation}
  Here the covariation in the second term vanished because \( \ol{M}^N \) is a càdlàg (semi)martingale and \( A^N \) is continuous and of finite variation~\cite[Cor. 8.5]{klebanerIntroductionStochasticCalculus2012}. % also https://math.stackexchange.com/questions/4974191/clarification-on-covariation-between-semimartingale-and-finite-variation-process
  The latter also explains why the quadratic variation of \( A^N \) in the last term vanished~\cite[Th. 1.10]{klebanerIntroductionStochasticCalculus2012}.
  Thus,
  \begin{equation}
    \sbr{\ol{M}^N}_t = \sbr{S^N}_t = M_t^{\lambda, N}[\varphi] - 2 \int_0^t S_{r-}^N \odif{\ol{M}_r^N} + \mathcal{O}(N^{-1}).
  \end{equation}
  Because \( ( S_{r-}^N )_r \) is predictable and \( ( \ol{M}_r^N )_r \) is a (local) martingale, the stochastic integral \( ( \int_0^s S_{r-}^N \odif{\ol{M}_r^N} )_s \) is a local martingale~\cite[218]{klebanerIntroductionStochasticCalculus2012}.
  In fact, since \( (S_{r-}^N)_r \) is uniformly bounded (by \( \norm{\psi}_\infty \)) and \( \ol{M}_t^N \) is also bounded, % as argued before
  it is a martingale~\cite[Cor. 7.22]{klebanerIntroductionStochasticCalculus2012}.
  It follows that
  \begin{equation}
    \begin{split}
      \Var{\lawmu^{\lambda,N}}{\ol{M}_t^N}
       & = \Exp{\lawmu^{\lambda,N}}{\sbr{\ol{M}^N}_t}                                                                                                           \\
       & = \Exp*{\lawmu^{\lambda,N}}{M_t^{\lambda, N}\sbr*{ \varphi }} - 2 \Exp*{\lawmu^{\lambda,N}}{\int_0^t S_{r-}^N \odif{\ol{M}_r^N}} + \mathcal{O}(N^{-1}) \\
       & = \mathcal{O}(N^{-1}) \overset{N\to\infty}{\to} 0. \qedhere
    \end{split}
  \end{equation}
\end{proof}
