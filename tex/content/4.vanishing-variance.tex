\section{Vanishing variance}    % TODO make subsection of previous section? Then the zcref below is unnecessary

In future sections the empirical martingale from \zcref{subsec:emp-mart} will be our workhorse to study limiting behavior of the neuronal process.
First however we will derive some of its properties.

We will show that the variance of \((\ol{M}_t^{\lambda,N}[\psi])_{t\in[0,T]}\) vanishes for large \(N\), implying that it has a deterministic limit.    % TODO does that imply that?

\begin{proposition}\label{prop:vanishing-variance}  % TODO could remove variance, and just show quad. var. goes to zero as that is all I need, right?
  For all \(\psi \in C_0^1(\potspace)\), \(\lambda > 0\), \(t \in [0,T]\),
  \begin{equation}
    \lim_{N\to\infty} \Var{\lawmu^{\lambda,N}}{\ol{M}_t^{\lambda,N}[\psi]} = 0.
  \end{equation}
\end{proposition}
\begin{proof}
  Throughout this proof we suppress the dependence of the empirical martingale on \( \psi \) and \( \lambda \), as both are fixed.
  Recall that \((\ol{M}_s^N)_{s\in[0,T]}\) is a martingale by \zcref{prop:emp-M-mart}.
  In particular,
  \begin{equation}
    \Exp{\lawmu^{\lambda,N}}{\ol{M}_t^N} = \Exp{\lawmu^{\lambda,N}}{\ol{M}_0^N}
    = \Exp{\lawmu^{\lambda,N}}{0} = 0.    % the second-to-last step is obvious from \zcref{eq:M-def}
  \end{equation}
  Note also that \({(\ol{M}_s^N)}_{s\in[0,T]}\) is bounded, so in particular square integrable.
  Hence the variance exists and equals the second moment.
  We can relate the second moment to the expectation of the quadratic variation using Theorem 7.27 from~\cite[201]{klebanerIntroductionStochasticCalculus2012}.
  Namely, since \(\ol{M}_0^N = 0\),
  \begin{equation}
    \Exp{\lawmu^{\lambda,N}}{(\ol{M}_t^N)^2} = \Exp{\lawmu^{\lambda,N}}{\sbr{\ol{M}^N}_t}.
  \end{equation}
  % \begin{equation}
  %     (\ol{M}_t^N)^2 = \abr{\ol{M}^N}_t + 2 \int_0^t \ol{M}_{s-}^N \odif{\ol{M}_s^N}
  % \end{equation}
  % Note that \((\ol{M}_s^N)_{s\in[0,T]}\) is càdlàg by \zcref{eq:M-def} and adapted, so \((\ol{M}_{s-}^N)_{s\in[0,T]}\) is left-continuous and adapted.
  % This means that it is predictable (see e.g. Definition 8.3 \cite{klebanerIntroductionStochasticCalculus2012}), so as \((\ol{M}_s^N)_{s\in[0,T]}\) is bounded the stochastic integral is well-defined.
  % NOTE comment on "bounded" above:
  % Cor de Leeuw
  % 29 November, 1:32 am
  % Add assumptions on b and domain of L and hat{L} to ensure this holds
  % Cor de Leeuw
  % 29 November, 1:38 am
  % Add above that this boundedness also implies that M is square integrable, so the second moment we are after exists

  Now define
  \begin{equation}
    S_s^N \coloneqq \int_\potspace \psi \odif{\mu_s^N},
    \quad
    A_s^{\lambda, N} \coloneqq \int_0^s \int_\potspace \ol{L}_{\mu_r^N}^{\lambda} [\psi] \odif{\mu_{r}^N} \odif{r}, \quad s \in [0,T].
  \end{equation}
  Just as for the empirical martingale, we will write \( A_s^N \coloneqq A_s^{\lambda, N} \) in this proof since \( \lambda \) is fixed.
  Then we have
  \begin{equation}\label{eq:M=S-A}
    \ol{M}_t^N = S_t^N - S_0^N - A_t^N.
  \end{equation}
  Note that \((A_s)_{s\in[0,T]}\) is a process of bounded variation, because it is absolutely continuous. % NOTE if we follow Klebaner's terminology and had [0,\infty] instead of [0,T], then I think it would only imply "finite variation" instead of "bounded variation"
  (This absolute continuity follows directly from the defining expression of \((A_s)\).) % which is of "fundamental theorem of calculus"-form.
  % Because \((A_s)\) is, in particular, continuous and adapted it is also predictable~\cite[215]{klebanerIntroductionStochasticCalculus2012}. % NOTE do I need this?
  Thus, \((S_s)_{s\in[0,T]}\) is the sum of a martingale and a process of bounded variation, making it a semimartingale.
  By Theorem 8.6 in~\cite[221]{klebanerIntroductionStochasticCalculus2012} it now follows that
  \begin{equation}\label{eq:S^2-q-var-rel}
    \begin{split}
      \sbr{S^N}_t & = \sbr{S^N - S_0^N}_t                                                                                \\   % this is clear from the definition of covariation: constants don't vary
                  & = (S_t^N - S_0^N)^2 - 2 \int_0^t (S_{r-}^N - S_0^N) \odif{\br{S_r^N - S_0^N}}                        \\
                  & = (S_t^N)^2 - 2 S_t^N S_0^N + (S_0^N)^2 - 2 \int_0^t (S_{r-}^N - S_0^N) \odif{S_r^N}                 \\   % again, constants don't vary so the don't matter when integrating against them
                  & = (S_t^N)^2 - 2 S_t^N S_0^N + (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{S_r^N} + 2 S_0^N (S_t^N - S_0^N) \\
                  & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{S_r^N}.
    \end{split}
  \end{equation}

  Now we are going to reuse the original martingale from \zcref{thm:mart-problem-sol}, but we will choose \(\varphi \coloneqq \br{ \frac{1}{N} \bigoplus_{i=1}^N \psi }^2\) as the test function this time.
  Recall \zcref{eq:M-def} and \zcref{eq:mart-problem-s-}, which together yield
  \begin{equation}
    M_t^{\lambda, N}[\varphi]
    = \varphi(\bm{X}_t) - \varphi(\bm{X}_0) - \int_0^t L^{\lambda, N} \varphi (\bm{X}_{r-}) \odif{r}
    = \varphi(\bm{X}_t) - \varphi(\bm{X}_0) - \int_0^t (L_{\cD}^{N} + L_{\cJ}^{\lambda, N}) \varphi (\bm{X}_{r-}) \odif{r}.
  \end{equation}
  We will substiture the aforementioned choice of \( \varphi \) into this expression term by term, starting with
  \begin{equation}
    \begin{split}
      \varphi(\bm{X}_t) - \varphi(\bm{X}_0)
       & = \br*{\frac{1}{N} \sum_{i=1}^N \psi(X_t^i)}^2 - \br*{\frac{1}{N} \sum_{i=1}^N \psi(X_0^i)}^2 \\
       & = \br*{ \int_\potspace \psi \odif{\mu_t^N} }^2 - \br*{ \int_\potspace \psi \odif{\mu_0^N} }^2
      = (S_t^N)^2 - (S_0^N)^2.
    \end{split}
  \end{equation}

  Next, we have
  \begin{equation}
    \begin{split}
      (L_{\cD}^{N} \varphi) (\bm{X}_{r-})
       & = \sum_{i=1}^N b(X_{r-}^i) \partial_{x^i}\br*{ \frac{1}{N} \bigoplus_{j=1}^N \psi }^2 (\bm{X}_{r-})               \\
       & = \frac{1}{N^2} \sum_{i=1}^N b(X_{r-}^i) \cdot 2 \br*{ \sum_{j=1}^N \psi(X_{r-}^j) } \cdot \partial_x \psi(X_{r-}^i) \\
       & = \frac{2}{N^2} \sum_{j=1}^N \psi(X_{r-}^j) \sum_{i=1}^N b(X_{r-}^i) \partial_x \psi(X_{r-}^i)                       \\
       & = 2 \int_\potspace \psi \odif{\mu_{r-}^N} \int_\potspace b(x) \partial_x \psi (x) \mu_{r-}^N(\odif{x})            \\
       & = 2 S_{r-}^N \int_\potspace b(x) \partial_x \psi (x) \mu_{r-}^N(\odif{x}).
    \end{split}
  \end{equation}

  Finally, similar to \zcref{eq:kappa-to-mean-kappa},
  \begin{equation}
    \begin{split}
      (L_{\cJ}^{\lambda, N} \varphi) (\bm{X}_{r-})
       & = \int_{\potspace^N} \sbr*{ \br*{ \frac{1}{N} \sum_{j=1}^N \psi(y^j) }^2 - \br*{ \frac{1}{N} \sum_{j=1}^N \psi(X_{r-}^j) }^2 } \lambda \kappa(\bm{X}_{r-},\odif{\bm{y}})                                                                   \\
       & = \sum_{i=1}^N \int_{\potspace^N} \sbr*{ \br*{ \frac{1}{N} \sum_{j=1}^N \psi(y^j) }^2 - \br*{ \frac{1}{N} \sum_{j=1}^N \psi(X_{r-}^j) }^2 } \lambda \delta_{\jtarg^i(\bm{X}_{r-})}(\odif{y^i}) \prod_{k \neq i} \delta_{X_{r-}^k}(\odif{y^k}) \\
      %
       & = \frac{\lambda}{N^2} \sum_{i=1}^N \sbr*{ \br*{  \sum_{\substack{j=1                                                                                                                                                                 \\ j \neq i}}^N \psi(X_{r-}^j) + \psi(\jtarg^i(\bm{X}_{r-})) }^2 - \br*{ \sum_{\substack{j=1 \\ j \neq i}}^N \psi(X_{r-}^j) + \psi(X_{r-}^i) }^2 }  \\
      %
      %  & = \frac{\lambda}{N^2} \sum_{i=1}^N \sbr*{ \br*{ \sum_{\substack{j=1 \\ j \neq i}}^N \psi(X_{r-}^j) }^2 + 2 \psi(\jtarg^i(\bm{X}_{r-})) \sum_{\substack{j=1 \\ j \neq i}}^N \psi(X_{r-}^i) + \psi^2(\jtarg^i(\bm{X}_{r-})) - \br*{ \sum_{\substack{j=1 \\ j \neq i}}^N \psi(X_{r-}^j) + \psi(X_{r-}^i) }^2 }^2 } \\
      %
      % Simply using (a + b)^2 - (a + c)^2 = a^2 + 2 a b + b^2 - ( a^2 + 2 a c + c^2 ) = 2 (b - c) a + b^2 - c^2
       & = \frac{\lambda}{N^2} \sum_{i=1}^N \sbr*{ 2 (\psi(\jtarg^i(\bm{X}_{r-})) - \psi(X_{r-}^i)) \br*{\sum_{\substack{j=1                                                                                                                        \\ j \neq i}}^N \psi(X_{r-}^j)} + \psi^2(\jtarg^i(\bm{X}_{r-})) - \psi^2(X_{r-}^i) } \\
      % Using the identity \jtarg^i(\bm{X}_{r-}) = \ol{\jtarg}_{\mu_{r-}^N}(X_{r-}^i), which holds due to the uniform, scaled weight choice
       & = \frac{\lambda}{N^2} \sum_{i=1}^N \sbr*{ 2 (\psi(\ol{\jtarg}_{\mu_{r-}^N}(X_{r-}^i)) - \psi(X_{r-}^i)) \br*{\sum_{j=1}^N \psi(X_{r-}^j) - \psi(X_{r-}^i)} + \psi^2(\ol{\jtarg}_{\mu_{r-}^N}(X_{r-}^i)) - \psi^2(X_{r-}^i) }                                 \\
      %
       & = \frac{1}{N^2} \sum_{i=1}^N \int_\potspace \sbr*{ 2 (\psi(y) - \psi(X_{r-}^i)) \br*{\sum_{j=1}^N \psi(X_{r-}^j) - \psi(X_{r-}^i)} + \psi^2(y) - \psi^2(X_{r-}^i) } \lambda \ol{\kappa}_{\mu_{r-}^N}(X_{r-}^i, \odif{y})                               \\
      %
       & = \frac{1}{N} \int_\potspace \int_\potspace \sbr*{ 2 (\psi(y) - \psi(x)) \br*{\sum_{j=1}^N \psi(X_{r-}^j)} - 2 (\psi(y) - \psi(x)) \psi(x) + \psi^2(y) - \psi^2(x) } \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) \mu_{r-}^N(\odif{x}).
    \end{split}
  \end{equation}
  Recall that \( \psi \) is bounded, so that in particular \( (x,y) \mapsto - 2 (\psi(y) - \psi(x)) \psi(x) + \psi^2(y) - \psi^2(x) \) is bounded, and that \( \ol{\kappa}_{\mu_{r-}^N} \) and \( \mu_{r-}^N \) are both finite (probability) measures.
  This implies that
  \begin{equation}
    \begin{split}
      (L_{\cJ}^{\lambda, N} \varphi) (\bm{X}_{r-})
       & = \frac{2}{N} \br*{\sum_{j=1}^N \psi(X_{r-}^j)}  \int_\potspace \int_\potspace (\psi(y) - \psi(x)) \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) \mu_{r-}^N(\odif{x}) + \bigO{N^{-1}} \\
       & = 2 \int_\potspace \psi \odif{\mu_{r-}^N} \int_\potspace \int_\potspace (\psi(y) - \psi(x)) \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) \mu_{r-}^N(\odif{x})  + \bigO{N^{-1}}       \\
       & = 2 S_{r-}^N \int_\potspace \int_\potspace (\psi(y) - \psi(x)) \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) \mu_{r-}^N(\odif{x})  + \bigO{N^{-1}},
    \end{split}
  \end{equation}
  for \( N \to \infty \).
  Combining the terms now yields
  \begin{equation}
    \begin{split}
      M_t^{\lambda, N}[\varphi]
                                         & = \varphi(\bm{X}_t) - \varphi(\bm{X}_0) - \int_0^t (L_{\cD}^{N} + L_{\cJ}^{\lambda, N}) \varphi (\bm{X}{r-}) \odif{r}                                                                                                                                             \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t \cbr*{ S_{r-}^N \int_\potspace b(x) \partial_x \psi (x) \mu_{r-}^N(\odif{x}) + S_{r-}^N \int_\potspace \int_\potspace (\psi(y) - \psi(x)) \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) \mu_{r-}^N(\odif{x})  + \bigO{N^{-1}} }  \odif{r} \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \int_\potspace \cbr*{ b(x) \partial_x \psi (x) + \int_\potspace (\psi(y) - \psi(x)) \lambda \ol{\kappa}_{\mu_{r-}^N}(x, \odif{y}) } \mu_{r-}^N(\odif{x})  \odif{r} + \bigO{N^{-1}}                                         \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \int_\potspace \ol{L}_{\mu_{r-}^N}^{\lambda} [\psi] \odif{\mu_{r-}^N} \odif{r} + \bigO{N^{-1}}                                                                                                                                  \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odv{A_{r-}^N}{r} \odif{r} + \bigO{N^{-1}}                                                                                                                                                                              \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{A_{r-}^N} + \bigO{N^{-1}}                                                                                                                                                                                         \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{A_r^N} + \bigO{N^{-1}}                                                                                                                                                                                         \\   % A^N is (absolutely) continuous
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{\br*{S_r^N - S_0^N - \ol{M}_r^N}} + \mathcal{O}(N^{-1})                                                                                                                                                        \\
                                         & = (S_t^N)^2 - (S_0^N)^2 - 2 \int_0^t S_{r-}^N \odif{S_r^N} + 2 \int_0^t S_{r-}^N \odif{\ol{M}_r^N} + \mathcal{O}(N^{-1})                                                                                                                                              \\
      \overset{\eqref{eq:S^2-q-var-rel}} & {=} \sbr{S^N}_t + 2 \int_0^t S_{r-}^N \odif{\ol{M}_r^N} + \mathcal{O}(N^{-1}),
    \end{split}
  \end{equation}

  Now, note that the first term equals \(\sbr{\ol{M}^N}_t\) because \(A\) is of bounded variation (so its quadratic variation is zero).
  The second term is a martingale so it vanishes in expectation.    % TODO explain more on why martingale? Note that S^N is bounded, and that both Klebaner and Jacod and Sirjaev say that the integrator being a local martingale implies that the integral is also a local martingale. As we are on [0,T], local martingales are just martingales I think
  Hence
  \begin{equation}
    \Var{\lawmu^{\lambda,N}}{\ol{M}_t^N}
    = \Exp{\lawmu^{\lambda,N}}{\sbr{\ol{M}^N}_t}
    = \Exp*{\lawmu^{\lambda,N}}{M_t^{\lambda, N}\sbr*{ \br*{ \frac{1}{N} \bigoplus_{i=1}^N \psi }^2 }} + \mathcal{O}(N^{-1})
    = \mathcal{O}(N^{-1}) \overset{N\to\infty}{\to} 0.
  \end{equation}
\end{proof}