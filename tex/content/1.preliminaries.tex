\section{Preliminaries}

Let \((E,d)\) be a metric space.
We will assume that \((E,d)\) is both separable and complete, because we will exclusively work with such spaces.

\subsection{Notation}
\begin{itemize}
  \item \(\cB(E)\) denotes the Borel \( \sigma \)-algebra on \(E\).
  \item \( \cB = \cB(\bR) \) denotes the Borel \( \sigma \)-algebra on the set of real numbers.
  \item \(M(E)\) is the space of Borel measurable, real-valued functions on \(E\).
  \item \(M_b(E) \subset M(E)\) is the space of functions which are additionally bounded.
  \item \(M_0(E) \subset M(E)\) is the space of functions which are additionally vanishing at infinity.
  \item \(M_c(E) \subset M(E)\) is the space of functions which are additionally compactly supported.
  \item \(C_*(E) \subset M_*(E)\), \( * \in \Set{b, 0, c} \) is the space of functions which are (additionally) continuous.
  \item \(C_*^{1-}(E) \subset C_*(E)\), \( * \in \Set{b, 0, c} \) is the space of functions which are (additionally) locally Lipschitz continuous.
  \item \(C_*^{k}(E) \subset C_*^{1-}(E)\), \( * \in \Set{b, 0, c} \), \( k \in \bN \) is the space of functions which are (additionally) \(k\)-times continuously differentiable.
  \item \(\cP(E)\) is the space of Borel probability measures on \(E\).
\end{itemize}

\subsection{The Skorokhod space}%, topology, metric and \texorpdfstring{\(\sigma\)}{sigma}-algebra}
The Skorokhod space \(D_E[0,\infty)\) is defined as the space of functions \(x : [0,\infty) \to E\) which are right-continuous and have existing left-limits.
That is, for \(x \in D_E[0,\infty)\) we have
\begin{equation}
  \forall t \in [0,\infty) : x(t+) \coloneqq \lim_{s \downarrow t} x(s) = x(t), \quad\text{and}\quad x(t-) \coloneqq \lim_{s \uparrow t} x(s)\ \text{exists},
\end{equation}
where the latter holds by convention for \(t = 0\).
Often, we will use subscripts to denote evaluation of elements of \(D_E[0,\infty)\), i.e., \(x_t \coloneqq x(t)\).

% TODO: could simplify to [0,T], and cite the simpler metric from Kouritzin here
We topologize the Skorokhod space \(D_E[0,\infty)\) using the (J1) Skorokhod metric as defined in~\cite[pp.116-117]{ethierMarkovProcessesCharacterization1985}.
This metric is defined as
\begin{equation}
  d_\mathrm{SK}(x,y) \coloneqq \inf_{\lambda \in \Lambda} \cbr*{ \norm{\lambda}_\circ \lor \int_0^\infty e^{-s} \widetilde{d}(x,y,\lambda,s) \odif{s} },
\end{equation}
where \( \Lambda \) is the set of Lipschitz continuous, strictly increasing bijections that map \([0,\infty)\) onto \([0,\infty)\) for which
\begin{equation}
  \norm{\lambda}_\circ \coloneqq \sup_{s > t \geq 0} \abs*{ \log \frac{\lambda(s)-\lambda(t)}{s-t} } < \infty,
\end{equation}
and
\begin{equation}
  \widetilde{d}(x,y,\lambda,s) \coloneqq \sup_{t \geq 0} \cbr{ 1 \land d(x(t \land s), y(\lambda(t) \land s)) }.
\end{equation}
Because \((E, d)\) is both complete and separable, so is \((D_E[0,\infty), d_\mathrm{SK})\)~\cite[Theorem 3.5.6]{ethierMarkovProcessesCharacterization1985}.

For an intuitive justification of the machinery involved in this metric, see~\cite{kernSkorokhodTopologiesWhat2024}.
Other choices of metric are possible but they induce the same topology, see~\cite[pp.166-168]{billingsleyConvergenceProbabilityMeasures1999} and~\cite[p.122-123]{pollardConvergenceStochasticProcesses1984}.

To characterize relative compactness under the Skorokhod metric a result similar to the Arzelà-Ascoli theorem exists.
It also makes use of a modulus of continuity, but then one that can avoid the discontinuities that càdlàg functions can have.
Let \( \cC_T^\delta \) be the collection of \( \delta \)-sparse partitions \(\Set{t_i}_{i=0}^n\), which satisfy \(0 = t_0 < t_1 < \cdots < t_n = T\) and \( \min_{i=1,\dots,n} t_i - t_{i-1} > \delta \).
Then define
\begin{equation}\label{eq:w-w'-def}
  \begin{split}
    w(x, [c,d))      & \coloneqq \sup_{c \leq s < t < d} d(x_t,x_s),                            \\
    w'(x, \delta, T) & \coloneqq \inf_{\Set{t_i} \in \cC_T^\delta} \max_i w(x, [t_{i-1}, t_i)).
  \end{split}
\end{equation}

\begin{theorem}\label{prelim:thm:rel-compact-in-SK}
  A set of paths \(A \subset D_E[0,\infty)\) is relatively compact (w.r.t.\@ \(d_\mathrm{SK}\)) if and only if
  \begin{enumerate}[(i)]
    \item For all \(T > 0\), there exists a compact set \(\Gamma_T \subset E\) such that \(x_t \in \Gamma_T\) for all \(t \in [0,T]\), \(x \in A\).
    \item For all \(T > 0\),
          \begin{equation}
            \lim_{\delta\to0} \sup_{x \in A} w'(x,\delta,T) = 0.
          \end{equation}
  \end{enumerate}
\end{theorem}

\begin{proof}
  This is Theorem 3.6.3 modified based on Remark 3.6.4 from Ethier and Kurz~\cite{ethierMarkovProcessesCharacterization1985}.
\end{proof}

We define \(\mathscr{S}_{E}\) to be the Borel \( \sigma \)-algebra induced by the Skorokhod topology.
Because \(E\) is separable, \(\mathscr{S}_{E}\) is generated by the point-evaluation maps~\cite[Proposition 3.7.1]{ethierMarkovProcessesCharacterization1985}.

\subsection{Narrow convergence}

Many different notions of convergence exist for sequences of (probability) measures.
We will focus on \textit{narrow convergence}, which is also often called weak convergence.
On the level of random variables it corresponds to convergence in distribution.

\begin{definition}\label{def:narrow-conv}
  Let \(\Set{\mu_n}_{n\in\bN} \subset \cP(E)\).
  This sequence of probability measures converges narrowly to \(\mu \in \cP(E)\), which we denote with \( \mu_n \wto \mu \), if
  \begin{equation}
    \forall \varphi \in C_b(E) : \lim_{n\to\infty} \int_E \varphi \odif{\mu_n} = \int_E \varphi \odif{\mu}.
  \end{equation}
\end{definition}

\begin{remark}
  This is just one possible definition of narrow convergence; there are several alternative, equivalent definitions.
  Together these form the Portmanteau theorem, see \cite[16]{billingsleyConvergenceProbabilityMeasures1999}.
\end{remark}

For a sequence of probability measures to converge narrowly, no \enquote{escape of mass} may occur.
This requirement is formalized through the following concept.

\begin{definition}
  A set of probability measures \(\Pi \subseteq \cP(E)\) is said to be \textit{tight} if
  \begin{equation}
    \forall \eps > 0 : \exists K \Subset E : \forall \mu \in \Pi : \mu(K) > 1 - \eps.
  \end{equation}
\end{definition}

Due to completeness and separability of \((E,d)\), a set consisting of a single probability measure is tight by Theorem 1.3 in~\cite{billingsleyConvergenceProbabilityMeasures1999}.
By extension, any finite \(\Pi \subseteq \cP(E)\) is tight as well (since a finite union of compact sets is compact).

Tightness is not just a necessary condition for convergence: it also implies the existence of a converging subsequence.
This is the subject of Prokhorov's theorem~\cite[57-65]{billingsleyConvergenceProbabilityMeasures1999}.

\begin{theorem}[Prokhorov]
  \( \Pi \subseteq \cP(E) \) is relatively compact if and only if it is tight.
\end{theorem}

\subsubsection{Tightness in Skorokhod spaces}

We want to apply Prokhorov's theorem to determine the existence of a subsequence in the case where \( \Pi = \Set{\mu_n}_{n\in\bN} \subset \cP(D_{E}[0,T]) \), i.e., a sequence of probability measures on the Skorokhod space consisting of paths in~\(E\).\footnote{In particular, we will consider \(E = \cP(\potspace)\).}
To this end, we will make use of a characterization of tightness specific to probability measures on Skorokhod spaces by Kouritzin~\cite{kouritzinTightnessProbabilityMeasures2015}.

There, the results are provided on the level of processes, but we will instead formulate them for the laws of those processes (which is entirely equivalent).
Kouritzin's setting is also very general, with \( E \) being a completely regular topological space.
We will restrict Kouritzin's results to our context, in which \((E,d)\) is a Polish metric space.

\begin{definition}[CCC]\label{def:CCC}
  Let \( \Pi \subseteq \cP(D_{E}[0,T]) \).
  Then \( \Pi \) satisfies the \textit{compact containment condition} if
  \begin{equation}
    \forall \eps > 0 : \exists K \Subset E : \inf_{\mu \in \Pi} \mu(\Set{x \in D_E[0,T] \given \forall t \in [0,T] : x_t \in K}) \geq 1 - \eps.
  \end{equation}
\end{definition}

\begin{definition}[MCC]\label{def:MCC}
  Let \( \Pi \subseteq \cP(D_{E}[0,T]) \).
  Then \( \Pi \) satisfies the \textit{modulus of continuity condition} if
  \begin{equation}\label{eq:MCC}
    \forall \eta > 0 : \exists \delta > 0 : \sup_{\mu \in \Pi} \mu(\Set{x \in D_E[0,T] \given w'(x,\delta) \geq \eta}) \leq \eta.
  \end{equation}
  It satisfies the \textit{asymptotic modulus of continuity condition} (AMCC) if for every \( \eta > 0 \) there exists a \( \delta > 0 \) and a finite set \( \Pi_0 \subset \Pi \) such that the statement in \zcref{eq:MCC} holds with \( \Pi \) replaced by \( \Pi \setminus \Pi_0 \).
\end{definition}

Note how these two conditions relate to the two conditions in \zcref{prelim:thm:rel-compact-in-SK}.
The following result is therefore not surprising.

\begin{theorem}
  Suppose \( \Pi \subseteq \cP(D_{E}[0,T]) \) satisfies both the CCC and the MCC.\@ Then \( \Pi \) is tight.
\end{theorem}

\begin{proof}
  See Theorem 13 from~\cite{kouritzinTightnessProbabilityMeasures2015}, or Theorem 3.7.2 plus Remark 3.7.3 from~\cite{ethierMarkovProcessesCharacterization1985}.
\end{proof}

However, verifying the CCC and/or the MCC can be difficult.
To that end, Kouritzin introduces weaker alternative conditions.
One can replace either the CCC or the MCC (not both) with such a weaker condition, and still preserve the tightness result.
We will only require the substitution of the MCC by a weaker alternative, which we will provide after defining a separation property for collections of functions.

\begin{definition}\label{def:s.p.}
  A set \( \cG \subseteq M(E) \) \textit{separates points} if for every pair of distinct points \( x, y \in E \) there exists a \( g \in \cG \) such that \( g(x) \neq g(y) \).
\end{definition}

\begin{definition}[WMCC]\label{def:WMCC}
  Let \( \Pi \subseteq \cP(D_{E}[0,T]) \).
  Then \( \Pi \) satisfies the \textit{weak modulus of continuity condition} if there exists a \( \cG \subseteq C(E) \) that separates points,\footnote{Kouritzin only requires separation of points to hold on compact sets, but this is equivalent to global separation of points (since any set \( \Set{x,y} \subseteq E \) is compact (in any topological space)).}
  % NOTE So the weakening to "on compacts" added by Kouritzin seems to be meaningless?
  and the set of pushforward measures \( \Set{g \sharp \mu \given \mu \in \Pi} \subseteq \cP(D_\bR[0,T]) \)\footnote{We abuse notation to identify \( g : E \to \bR \) with the map \( D_E[0,T] \mapsto D_\bR[0,T] \) that applies \( g \) pointwise.} satisfies the AMCC for all \( g \in \cG \cup \Set{f + h \given f, h \in \cG} \).
  (The metric on \( \bR \) is induced by the usual Euclidean norm \( \abs{\cdot} \).)
  % and \( \Set{g \circ X^\alpha} \) satisfies the AMCC on \( (\bR, \abs{\cdot}) \) for all \( g \in \cG \cup \Set{f + h \given f, h \in \cG} \).
\end{definition}

By Theorem 20 from Kourizin \cite{kouritzinTightnessProbabilityMeasures2015}, we can still achieve tightness with this weaker condition.

\begin{theorem}\label{thm:ccc+wmcc=tight}
  If \( \Pi \subseteq \cP(D_{E}[0,T]) \) satisfies the CCC and the WMCC, then it is tight.
\end{theorem}
% NOTE: Theorem 20 in Kouritzin is (again) formulated for processes, and it yields tightness only for indistinguishable processes. But indistinguishable processes have the same law, so I don't understand why that is different from just X^alpha being tight?

\subsection{(Semi)martingales}

In \zcref{sec:} we will use various concepts and tools from stochastic calculus, which we will briefly enumerate here.
A more detailed explanation can be found in~\cite{klebanerIntroductionStochasticCalculus2012}.

\begin{definition}
  A stochastic process \(\Set{M_t}_{t}\) is a \textit{martingale} if it is adapted to a filtration \(\Set{\cF_t}_t\), \( M_t \) is integrable for every \(t\), and for each \( s < t \) it satisfies
  \begin{equation}
    \Exp{}{M_t \given \cF_s} = M_s \quad \text{a.s.}
  \end{equation}
  It is a \textit{local martingale} if there exists an a.s.\ increasing sequence of stopping times \( \tau_n \) with \( \tau_n \uparrow \infty \) a.s.\ such that the stopped process \( M_{t \land \tau_n} \) is a martingale for all \( n \in \bN \). % NOTE: Klebaner 2012 requires uniform integrability, but by Proposition 4.4 in the Stochastic Integration lecture notes this is equivalent
\end{definition}

Of course, a martingale is also a local martingale.
This follows by choosing \( \tau_n \coloneqq n \), and using that a stopped martingale is still a martingale.

\begin{definition}
  The \textit{variation} of a real-valued function \( g \) that is defined on an interval \( [a,b] \) is equal to
  \begin{equation}
    V_g([a,b]) \coloneqq \sup \sum_{i=1}^n \abs{ g(t_i^n) - g(t_{i-1}) }
  \end{equation}
  where we take the supremum over all possible partitions \(\Set{(t_i^n)_{i=1,\dots,n}}_{n\in\bN}\) of the interval \( [a,b] \).
  The function \( g \) is of \textit{finite variation} if its variation over any bounded interval is finite.
  % It is of \textit{bounded variation} if \( \sup_t V_g([0,t]) < \infty \).    % Only define this if needed (otherwise confusing)
\end{definition}

\begin{definition}
  A stochastic process \(\Set{S_t}_{t}\) with càdlàg sample paths adapted to a filtration \(\Set{\cF_t}_t\) is called a \textit{semimartingale} if it can be decomposed as
  \begin{equation}
    S_t = S_0 + M_t + A_t,
  \end{equation}
  where \( \Set{M_t}_{t} \) is a local martingale w.r.t. \(\Set{\cF_t}_t\) and \( \Set{A_t}_{t} \) is a process whose paths are a.s. of finite variation, and \( M_0 = A_0 = 0 \) a.s.
\end{definition}

\begin{definition}
  If it exists, the \textit{quadratic variation} of a stochastic process \(\Set{X_t}_{t}\) equals
  \begin{equation}
    [X]_t \coloneqq \lim \sum_{i=1}^n (X_{t_i}^n - X_{t_{i-1}}^n)^2
  \end{equation}
  where the limit is a limit in probability over shrinking partitions \(\Set{(t_i^n)_{i=1,\dots,n}}_{n\in\bN}\).
\end{definition}

% TODO: It exists for semimartingales by Klebaner p.220


\subsection{Martingale problem on \texorpdfstring{\(D_E[0,\infty)\)}{D([0,infinity), E)}}\label{sec:martingale-problem}

Problems involving stochastic dynamics are often formulated as a \textit{martingale problem}.
For our definition of the martingale problem, we will follow Ethier and Kurz~\cite[p.174]{ethierMarkovProcessesCharacterization1985}.
Let \((A, D(A)) : M_b(E) \to M_b(E)\) be a linear operator.\footnote{The definition can be extended to multivalued, nonlinear operators, but we will not require such generality.}
We say that a probability measure \(P \in \cP(D_E[0,\infty))\) is a solution of the martingale problem for \(A\) if for the canonical coordinate process
\begin{equation}
  X_t(\omega) \coloneqq \omega_t, \quad \omega \in D_E[0,\infty), t \geq 0,
\end{equation}
defined on the probability space \((D_E[0,\infty), \mathscr{S}_E, P)\), it holds that for all test functions \(f \in D(A)\)
\begin{equation}\label{eq:mart-problem}
  t \mapsto f(X_t) - f(X_0) - \int_0^t (Af)(X_s) \odif{s}
\end{equation}
is a martingale with respect to the filtration \(\Set{\cF_t^X}_{t\geq0} \coloneqq \Set{\sigma(X_s : s \leq t)}\). % equals \mathscr{S}_E restricted to [0,t], see citation of Prop. 3.7.1 above
This eponymous condition is what makes the problem relevant in the study of stochastic processes.
It is satisfied whenever \(A\) is the generator of a Markov process \(X\)~\cite[161-162]{ethierMarkovProcessesCharacterization1985}.

If additionally \( X_0 \sharp P = P X_0^{-1} = \mu \) for a prescribed initial distribution \(\mu \in \cP(E)\), we say that \(P\) is a solution of the martingale problem for \((A,\mu)\).
Such a solution is called unique if for any two solutions the finite-dimensional distributions (of \(X\)) are identical.
The martingale problem for \((A,\mu)\) is well-posed if (i) a solution exists and (ii) it is unique.
If this holds for all \(\mu \in \cP(E)\), we say that the martingale problem for \(A\) is well-posed.

Also, because the canonical coordinate process is càdlàg it has at most countably many discontinuities.
% See https://math.stackexchange.com/questions/441721/the-set-of-jumps-of-a-c%C3%A0dl%C3%A0g-function-is-countable
This implies that
\begin{equation}\label{eq:mart-problem-s-}
  t \mapsto f(X_t) - f(X_0) - \int_0^t (Af)(X_s) \odif{s}
  = f(X_t) - f(X_0) - \int_0^t (Af)(X_{s-}) \odif{s},
\end{equation}
which is thus an equivalent formulation of the martingale problem.
