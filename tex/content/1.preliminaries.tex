\section{Preliminaries}

Let \((E,d)\) be a metric space.
We will assume that \((E,d)\) is both separable and complete, because we will exclusively work with such spaces.

\subsection{Notation}
\begin{itemize}
  \item \( \cB = \cB_{\bR} \) denotes the Borel \( \sigma \)-algebra on the set of real numbers.
  \item \(M(E)\) is the space of Borel measurable, real-valued functions on \(E\).
  \item \(M_b(E) \subset M(E)\) is the space of functions which are additionally bounded.
  \item \(M_0(E) \subset M(E)\) is the space of functions which are additionally vanishing at infinity.
  \item \(M_c(E) \subset M(E)\) is the space of functions which are additionally compactly supported.
  \item \(C_*(E) \subset M_*(E)\), \( * \in \Set{b, 0, c} \) is the space of function which are (additionally) continuous.
  \item \(C_*^{1-}(E) \subset C_*(E)\), \( * \in \Set{b, 0, c} \) is the space of function which are (additionally) locally Lipschitz continuous.
  \item \(C_*^{k}(E) \subset C_*^{1-}(E)\), \( * \in \Set{b, 0, c} \), \( k \in \bN \) is the space of function which are (additionally) \(k\)-times continuously differentiable.
  \item \(\cB(E)\) denotes the Borel \( \sigma \)-algebra on \(E\).
  \item \(\cP(E)\) is the space of Borel probability measures on \(E\).
\end{itemize}

\subsection{The Skorokhod space}%, topology, metric and \texorpdfstring{\(\sigma\)}{sigma}-algebra}
The Skorokhod space \(D_E[0,\infty)\) is defined as the space of functions \(x : [0,\infty) \to E\) which are right-continuous and have existing left-limits.
That is, for \(x \in D_E[0,\infty)\) we have
\begin{equation}
  \forall t \in [0,\infty) : x(t+) \coloneqq \lim_{s \downarrow t} x(s) = x(t), \quad\text{and}\quad x(t-) \coloneqq \lim_{s \uparrow t} x(s)\ \text{exists},
\end{equation}
where the latter holds by convention for \(t = 0\).
Often, we will use subscripts to denote evaluation of elements of \(D_E[0,\infty)\), i.e., \(x_t \coloneqq x(t)\).

% TODO: could simplify to [0,T], and cite the simpler metric from Kouritzin here
We topologize the Skorokhod space \(D_E[0,\infty)\) using the (J1) Skorokhod metric as defined in~\cite[pp.116-117]{ethierMarkovProcessesCharacterization1985}.
This metric is defined as
\begin{equation}
  d_\mathrm{SK}(x,y) \coloneqq \inf_{\lambda \in \Lambda} \cbr*{ \norm{\lambda}_\circ \lor \int_0^\infty e^{-s} \tilde{d}(x,y,\lambda,s) \odif{s} },
\end{equation}
where \(\Lambda\) is the set of Lipschitz continuous, strictly increasing bijections that map \([0,\infty)\) onto \([0,\infty)\) for which
\begin{equation}
  \norm{\lambda}_\circ \coloneqq \sup_{s > t \geq 0} \abs*{ \log \frac{\lambda(s)-\lambda(t)}{s-t} } < \infty,
\end{equation}
and
\begin{equation}
  \tilde{d}(x,y,\lambda,s) \coloneqq \sup_{t \geq 0} \cbr{ 1 \land d(x(t \land s), y(\lambda(t) \land s)) }.
\end{equation}
Because \((E, d)\) is both complete and separable, so is \((D_E[0,\infty), d_\mathrm{SK})\)~\cite[Theorem 3.5.6]{ethierMarkovProcessesCharacterization1985}.

For an intuitive justification of the machinery in this metric, see~\cite{kernSkorokhodTopologiesWhat2024}.
Other choices of metric are possible but they induce the same topology, see~\cite[pp.166-168]{billingsleyConvergenceProbabilityMeasures1999} and~\cite[p.122-123]{pollardConvergenceStochasticProcesses1984}.

To characterize compactness under this metric a result similar to the Arzelà-Ascoli theorem exists.
It also makes use of a modulus of continuity, but then one that can avoid the discontinuities that càdlàg functions can have.
Let \(\cC_T^\delta\) be the collection of partitions \(\Set{t_i}_{i=0}^n\) satisfying \(0 = t_0 < t_1 < \cdots < t_n = T\) and \(\min_{i=1,\dots,n} t_i - t_{i-1} > \delta\).
Then define
\begin{equation}
  w'(x, \delta, T) \coloneqq \inf_{\Set{t_i} \in \cC_T^\delta} \max_i \sup_{t_{i-1} \leq s < t < t_i} d(x_t,x_s).
\end{equation}

\begin{theorem}\label{prelim:thm:rel-compact-in-SK}
  A set of paths \(A \subset D_E[0,\infty)\) is relatively compact (w.r.t.\@ \(d_\mathrm{SK}\)) if and only if
  \begin{enumerate}[(i)]
    \item For all \(T > 0\), there exists a compact set \(\Gamma_T \subset E\) such that \(x_t \in \Gamma_T\) for all \(t \in [0,T]\), \(x \in A\).
    \item For all \(T > 0\),
          \begin{equation}
            \lim_{\delta\to0} \sup_{x \in A} w'(x,\delta,T) = 0.
          \end{equation}
  \end{enumerate}
\end{theorem}

\begin{proof}
  This is Theorem 3.6.3 modified based on Remark 3.6.4 from Ethier and Kurz~\cite{ethierMarkovProcessesCharacterization1985}.
\end{proof}

We define \(\mathscr{S}_{E}\) to be the Borel \(\sigma\)-algebra induced by the Skorokhod topology.
Because \(E\) is separable, \(\mathscr{S}_{E}\) is generated by the point-evaluation maps~\cite[Proposition 3.7.1]{ethierMarkovProcessesCharacterization1985}.

\subsection{Narrow convergence}

Many different notions of convergence exist for sequences of (probability) measures.
We will focus on \textit{narrow convergence}, which is also often called weak convergence.
On the level of random variables it corresponds to convergence in distribution.

\begin{definition}\label{def:narrow-conv}
  Let \(\Set{\mu_n}_{n\in\bN} \subset \cP(E)\).
  This sequence of probability measures converges narrowly to \(\mu \in \cP(E)\), which we denote with \( \mu_n \wto \mu \), if
  \begin{equation}
    \forall \varphi \in C_b(E) : \lim_{n\to\infty} \int_E \varphi \odif{\mu_n} = \int_E \varphi \odif{\mu}.
  \end{equation}
\end{definition}

\begin{remark}
  This is just one possible definition of narrow convergence; there are several alternative, equivalent definitions.
  Together these form the Portmanteau theorem, see \cite[16]{billingsleyConvergenceProbabilityMeasures1999}.
\end{remark}

For a sequence of probability measures to converge narrowly, no \enquote{escape of mass} may occur.
This requirement is formalized through the following concept.

\begin{definition}
  A set of probability measures \(\Pi \subseteq \cP(E)\) is said to be \textit{tight} if
  \begin{equation}
    \forall \eps > 0 : \exists K \Subset E : \forall \mu \in \Pi : \mu(K) > 1 - \eps.
  \end{equation}
\end{definition}

Tightness is not just a necessary condition for convergence; it also implies the existence of a converging subsequence.
This is the subject of Prokhorov's theorem~\cite[57-65]{billingsleyConvergenceProbabilityMeasures1999}.

\begin{theorem}[Prokhorov]
  \( \Pi \subseteq \cP(E) \) is relatively compact if and only if it is tight.
\end{theorem}

\subsubsection{Tightness in Skorokhod spaces}

We want to apply Prokhorov's theorem to determine the existence of a subsequence in the case where \( \Pi = \Set{\mu_n}_{n\in\bN} \subset \cP(D_{E}[0,T]) \), i.e., a sequence of probability measures on the Skorokhod space consisting of paths in \(E\).\footnote{In particular, we will consider \(E = \cP(\potspace)\).}
To this end, we will make use of a characterization of tightness specific to probability measures on Skorokhod spaces by Kouritzin~\cite{kouritzinTightnessProbabilityMeasures2015}.
There, the setting is very general, with \(E\) being a completely regular topological space.
We will restrict Kouritzin's result to our context, in which \((E,d)\) is a Polish metric space and the tight set is always a sequence.

\begin{definition}[CCC]
  Let \( \Pi = \Set{\mu_n}_{n\in\bN} \subset \cP(D_{E}[0,T]) \).
  Then \(\Pi\) satisfies the \textit{compact containment condition} if
  \begin{equation}
    \forall \eps > 0 : \exists K \Subset E : \inf_{n\in\bN} \mu_n(\Set{x \in D_E[0,T] \given \forall t \in [0,T] : x_t \in K}) \geq 1 - \eps.
  \end{equation}
\end{definition}

\begin{definition}[MCC]
  Let \( \Pi = \Set{\mu_n}_{n\in\bN} \subset \cP(D_{E}[0,T]) \).
  Then \(\Pi\) satisfies the \textit{modulus of continuity condition} if
  \begin{equation}
    \forall \eta > 0 : \exists \delta > 0 : \sup_{n\in\bN} \mu_n(\Set{x \in D_E[0,T] \given w'(x,\delta) \geq \eta}) \leq \eta.
  \end{equation}
\end{definition}

Note how these two conditions relate to the two conditions in \zcref{prelim:thm:rel-compact-in-SK}.
The following result is therefore not surprising.

\begin{theorem}
  Suppose \( \Pi = \Set{\mu_n}_{n\in\bN} \subset \cP(D_{E}[0,T]) \) satisfies both the CCC and the MCC.\@ Then \( \Pi \) is tight.
\end{theorem}

\begin{proof}
  See Theorem 13 from~\cite{kouritzinTightnessProbabilityMeasures2015}, or Theorem 3.7.2 plus Remark 3.7.3 from~\cite{ethierMarkovProcessesCharacterization1985}.
\end{proof}

However, verifying the CCC and/or the MCC can be difficult.
To that end, Kouritzin introduces weaker alternative conditions.
One can substitute either the CCC or the MCC (not both) with such a weaker condition, and still preserve the result.

% TODO: list the conditions I actually end up using

\subsection{Martingale problem on \texorpdfstring{\(D_E[0,\infty)\)}{D([0,infinity), E)}}\label{sec:martingale-problem}

Problems involving stochastic dynamics are often formulated as a \textit{martingale problem}.
For our definition of the martingale problem, we will follow Ethier and Kurz~\cite[p.174]{ethierMarkovProcessesCharacterization1985}.
Let \((A, D(A)) : M_b(E) \to M_b(E)\) be a linear operator.\footnote{The definition can be extended to multivalued, nonlinear operators, but we will not require such generality.}
We say that a probability measure \(P \in \cP(D_E[0,\infty))\) is a solution of the martingale problem for \(A\) if for the canonical coordinate process
\begin{equation}
  X_t(\omega) \coloneqq \omega_t, \quad \omega \in D_E[0,\infty), t \geq 0,
\end{equation}
defined on the probability space \((D_E[0,\infty), \mathscr{S}_E, P)\), it holds that for all test functions \(f \in D(A)\)
\begin{equation}
  t \mapsto f(X_t) - f(X_0) - \int_0^t (Af)(X_s) \odif{s}
\end{equation}
is a martingale with respect to the filtration \(\Set{\cF_t^X}_{t\geq0} \coloneqq \Set{\sigma(X_s : s \leq t)}\). % equals \mathscr{S}_E restricted to [0,t], see citation of Prop. 3.7.1 above
This eponymous condition is what makes the problem relevant in the study of stochastic processes.
It is satisfied whenever \(A\) is the generator of a Markov process \(X\)~\cite[161-162]{ethierMarkovProcessesCharacterization1985}.

If additionally \( (X_0)_\# P = P X_0^{-1} = \mu \) for a prescribed initial distribution \(\mu \in \cP(E)\), we say that \(P\) is a solution of the martingale problem for \((A,\mu)\).
Such a solution is called unique if for any two solutions the finite-dimensional distributions (of \(X\)) are identical.
The martingale problem for \((A,\mu)\) is well-posed if (i) a solution exists and (ii) it is unique.
If this holds for all \(\mu \in \cP(E)\), we say that the martingale problem for \(A\) is well-posed.
