\section{Preliminaries}

Let \((E, d)\) be a metric space.
We will assume that \((E, d)\) is both separable and complete, because we will exclusively work with such spaces.

\subsection{Notation}
\begin{itemize}
  \item \(\cB(E)\) denotes the Borel \( \sigma \)-algebra on \(E\).
  \item \( \cB = \cB(\bR) \) denotes the Borel \( \sigma \)-algebra on the set of real numbers.
  \item \(M(E)\) is the space of Borel measurable, real-valued functions on \(E\).
  \item \(M_b(E) \subset M(E)\) is the space of functions which are additionally bounded.
  \item \(M_0(E) \subset M(E)\) is the space of functions which are additionally vanishing at infinity.
  \item \(M_c(E) \subset M(E)\) is the space of functions which are additionally compactly supported.
  \item \(C_*(E) \subset M_*(E)\), \( * \in \Set{b, 0, c} \) is the space of functions which are (additionally) continuous.
  \item \(C_*^{1-}(E) \subset C_*(E)\), \( * \in \Set{b, 0, c} \) is the space of functions which are (additionally) locally Lipschitz continuous.
  \item \(C_*^{k}(E) \subset C_*^{1-}(E)\), \( * \in \Set{b, 0, c} \), \( k \in \bN \) is the space of functions which are (additionally) \(k\)-times continuously differentiable.
  \item \(\cP(E)\) is the space of Borel probability measures on \(E\).
  \item \( \land \) and \( \lor \) are minimum and maximum (binary) operators respectively.
  \item \( \Subset \) denotes compact subset.
\end{itemize}


\subsection{The Skorokhod space} %, topology, metric and \texorpdfstring{\(\sigma\)}{sigma}-algebra}
Given a fixed time horizon \( T > 0 \), the Skorokhod space \( D_E[0,T] \) is defined as the space of functions \(x : [0,T] \to E\) which are right-continuous and have existing left-limits (càdlàg).
That is, \(x \in D_E[0,T]\) is equivalent to
\begin{equation}
  \forall t \in [0,T) : x(t+) \coloneqq \lim_{s \downarrow t} x(s) = x(t), \quad\text{and}\quad \forall t \in (0,T] : x(t-) \coloneqq \lim_{s \uparrow t} x(s)\ \text{exists}.
\end{equation}
Often, we will use subscripts to abbreviate evaluation of such functions, i.e., \(x_t \coloneqq x(t)\).

We topologize the Skorokhod space \( D_E[0,T] \) using the complete (\( J_1 \)) Skorokhod metric as defined in~\cite[125]{billingsleyConvergenceProbabilityMeasures1999}.\footnote{There the metric is formulated for \( E = \bR \), but as they state on page 121 it can be generalized as we do here.}
This metric is defined as
\begin{equation}\label{eq:d_T-def}
  d_T^\mathrm{SK} (x,y) \coloneqq \inf_{\lambda \in \Lambda[0,T]} \br*{ \norm{\lambda}_T^\circ \lor \sup_{t \in [0,T]} d(x(\lambda(t)), y(t)) }, \quad x, y \in D_E[0,T],
  % Kouritzin's metric (simplified to using 1 metric d) and Kern's one are surely equivalent and both complete, as they both use the slope of lambda.
\end{equation}
where \( \Lambda[0,T] \) is the set of Lipschitz continuous, strictly increasing bijections that map \( [0,T] \) onto itself, and
\begin{equation}\label{eq:lambda-norm}
  \norm{\lambda}_T^\circ \coloneqq \sup_{0 \leq s < t \leq T} \abs*{ \log \frac{\lambda(t)-\lambda(s)}{t-s} }
  = \esssup_{0 \leq t \leq T} \abs{ \log \lambda'(t) }.    % NOTE: eq. (3.5.1) in EK. This holds because (i) lambda is a.e. diffable by being Lipschitz, (ii) the supremum over the fraction is >= the derivative by limit def. and (iii) the supremum over derivative is >= the fraction by the fact that lambda is absolutely continuous (as it is Lipschitz)
\end{equation}
Here \( \lambda \) should be interpreted as a `change of time', with the degree of change (measured based on \( \lambda \)'s slope) being penalized through \( \norm{\lambda}_\infty^\circ \).
This temporal flexibility ensures that \( x_n(t) \coloneqq x(t + \frac{1}{n}) \) converges to \( x(t) \coloneqq \indE{[0,1]}(t) \)\footnote{Assuming \( T > 1 \).} under \( d_\infty^\mathrm{SK} \), despite the discontinuity of \( x \) at \( t = 1 \).
(Crucially, this does not hold under the uniform metric \( d_\mathrm{unif}(x,y) \coloneqq \sup_{t \geq 0} d(x(t), y(t)) \).)
For more details on and justification of the machinery involved in the Skorokhod metric, see~\cite{kernSkorokhodTopologiesWhat2024}.

\begin{lemma}
  For any \( \lambda \in \Lambda[0,T] \),~\eqref{eq:lambda-norm} holds.
\end{lemma}
\begin{proof}
  Because \( \lambda \) is Lipschitz continuous and strictly increasing it has a positive derivative almost everywhere, so both expressions are well-defined.
  It remains to be shown that they are equal, i.e.,
  \begin{equation}\label{eq:lambda-norm-claim}
    \sup_{0 \leq s < t \leq T} \abs*{ \log \frac{\lambda(t)-\lambda(s)}{t-s} } = \esssup_{0 \leq r \leq T} \abs{ \log \lambda'(r) }.
  \end{equation}
  If \( \lambda'(r) \) exists, we have
  \begin{equation}
    \log \lambda'(r) = \log \lim_{t \to r} \frac{\lambda(t)-\lambda(r)}{t-r}
    \leq \log \sup_{0 \leq s < t \leq T} \frac{\lambda(t)-\lambda(s)}{t-s}
    % = \sup_{0 \leq s < t \leq T} \log \frac{\lambda(t)-\lambda(s)}{t-s}
    \leq \sup_{0 \leq s < t \leq T} \abs*{ \log \frac{\lambda(t)-\lambda(s)}{t-s} },
  \end{equation}
  by monotonicity of the logarithm.
  Analogously, it follows that the same upper bound holds for \( -\log \lambda'(r) \), so
  \begin{equation}\label{eq:lambda-norm-claim-geq}
    \esssup_{0 \leq r \leq T} \abs{ \log \lambda'(r) } \leq \sup_{0 \leq s < t \leq T} \abs*{ \log \frac{\lambda(t)-\lambda(s)}{t-s} }.
  \end{equation}
  Furthermore, as \( \lambda \) is Lipschitz continuous it is in particular absolutely continuous, implying that for \( s < t \)
  \begin{equation}
    \log \frac{\lambda(t)-\lambda(s)}{t-s} = \log \br*{ \frac{1}{t-s} \int_s^t \lambda'(\tau) \odif{\tau} }
    \leq \log \esssup_{0 \leq r \leq T} \lambda'(s)
    % = \esssup_{0 \leq r \leq T} \log \lambda'(s)
    \leq \esssup_{0 \leq r \leq T} \abs{ \log \lambda'(s) }.
  \end{equation}
  % \begin{equation}
  %   -\log \frac{\lambda(t)-\lambda(s)}{t-s} = \log \br*{ \frac{t-s}{\int_s^t \lambda'(\tau) \odif{\tau}} }
  %   \leq \log \frac{1}{\essinf_{0 \leq r \leq T} \lambda'(s)}
  %   = -\log \essinf_{0 \leq r \leq T} \lambda'(s)
  %   = \esssup_{0 \leq r \leq T} -\log \lambda'(s).
  % \end{equation}
  Again, the same upper bound can be shown to hold for \( -\log \frac{\lambda(t)-\lambda(s)}{t-s} \) in the same way, yielding
  \begin{equation}
    \sup_{0 \leq s < t \leq T} \abs*{ \log \frac{\lambda(t)-\lambda(s)}{t-s} } \leq \esssup_{0 \leq r \leq T} \abs{ \log \lambda'(r) },
  \end{equation}
  which combined with~\eqref{eq:lambda-norm-claim-geq} implies~\eqref{eq:lambda-norm-claim}.
\end{proof}

Because \((E, d)\) is both complete and separable, so is \((D_E[0,T], d_T^\mathrm{SK})\)~\cite[Theorem 12.2]{billingsleyConvergenceProbabilityMeasures1999}.
We define \(\mathscr{S}_T^E\) to be the Borel \( \sigma \)-algebra induced by \( d_T^\mathrm{SK} \).


\subsubsection{On unbounded time intervals}

Most of our results will be stated on the space \( D_E[0,T] \), but some intermediate steps will make use of the space \( D_E[0,\infty) \), which consists of the càdlàg functions that are defined for all nonnegative times.
Whenever we have an element of \( D_E[0,\infty) \), we can of course restrict its domain to obtain an element of \( D_E[0,T] \).

The topology on \( D_E[0,\infty) \) will be the one induced by the metric from~\cite[116-117]{ethierMarkovProcessesCharacterization1986}, which is defined as
\begin{equation}
  d_\infty^\mathrm{SK} (x,y) \coloneqq \inf_{\lambda \in \Lambda[0,\infty)} \cbr*{ \norm{\lambda}_\infty^\circ \lor \int_0^\infty e^{-s} \widetilde{d}(x,y,\lambda,s) \odif{s} }, \quad x, y \in D_E[0,\infty),
\end{equation}
where \( \Lambda[0,\infty) \) and \( \norm{\cdot}_\infty^\circ \) are straightforward adaptations of \( \Lambda[0,T] \) and \( \norm{\cdot}_T^\circ \), and
\begin{equation}
  \widetilde{d}(x,y,\lambda,s) \coloneqq \sup_{t \geq 0} \cbr{ 1 \land d(x(t \land s), y(\lambda(t) \land s)) }.
\end{equation}
Under the topology induced by this metric, \( x_n \to x \) in \( D_E[0,\infty) \) is more or less (see \zcref{cav:D_T-D_inf-equiv} below) equivalent to \( x_n \to x \) holding in \( D_E[0,T] \) for all \( T > 0 \) at which \( x \) is continuous; see~\cite[Proposition 3.5.2]{ethierMarkovProcessesCharacterization1986} and~\cite[Theorem 16.2]{billingsleyConvergenceProbabilityMeasures1999}.

\begin{caveat}\label{cav:D_T-D_inf-equiv}
  This equivalence holds exactly in~\cite[Theorem 16.2]{billingsleyConvergenceProbabilityMeasures1999}, but it is unclear if their Skorokhod topology on \( D_E[0,\infty) \) (induced by their metric \( d_\infty^\circ \) defined on pages 166-168) is the same as the one used by~\cite{ethierMarkovProcessesCharacterization1986} (which we use here).
  There is even a third possible definition of the Skorokhod metric on \( D_E[0,\infty) \), which is given in~\cite[p.122-123]{pollardConvergenceStochasticProcesses1984}.
\end{caveat}

Because \((E, d)\) is both complete and separable, so is \((D_E[0,\infty), d_\infty^\mathrm{SK})\)~\cite[Theorem 3.5.6]{ethierMarkovProcessesCharacterization1986}.
We define \(\mathscr{S}_\infty^E\) to be the Borel \( \sigma \)-algebra induced by \( d_\infty^\mathrm{SK} \).
% Because \(E\) is separable, \(\mathscr{S}_\infty^E\) is generated by the point-evaluation maps~\cite[Proposition 3.7.1]{ethierMarkovProcessesCharacterization1986}.


\subsubsection{Relative compactness}

Given a sequence of functions, the first step towards showing their convergence is often establishing relative compactness.
For continuous functions under the uniform metric this can be done with the well-known Arzelà-Ascoli theorem.
This theorem can be formulated using the \textit{modulus of continuity}, defined as
\begin{equation}
  w(x, \delta) \coloneqq \sup_{\abs{t - s} \leq \delta} d(x_t, x_s), \quad x \in C_E[0,T],\ \delta > 0.
\end{equation}

\begin{theorem}(Arzelà-Ascoli)
  A set of paths \( A \subseteq C_E[0,T] \) is relatively compact w.r.t.\@ the uniform metric if and only if
  \begin{enumerate}[(i)]
    % \item It is pointwise relatively compact: for all \( t \in [0,T] \) it holds that the set \( \Set{x_t \given x \in A} \subseteq E \) is relatively compact.
    \item there exists a compact set \(K \Subset E\) such that \(x_t \in K\) for all \(t \in [0,T]\), \(x \in A\); and
    \item it is (uniformly) \textit{equicontinuous}:
    \begin{equation}
      \lim_{\delta\to0} \sup_{x \in A} w(x,\delta) = 0.
    \end{equation}
  \end{enumerate}
\end{theorem}

To characterize relative compactness on \( D_E[0,T] \), a result very similar to the Arzelà-Ascoli theorem exists.
It uses a modification of the modulus of continuity \( w' \), which is designed to be able to avoid the discontinuities that càdlàg functions can have.
Let \( \cC_T^\delta \) be the collection of \( \delta \)-sparse partitions \(\Set{t_i}_{i=0}^n\), i.e., partitions that satisfy \(0 = t_0 < t_1 < \cdots < t_n = T\) and \( \min_{i=1,\dots,n} t_i - t_{i-1} > \delta \).
Then define, for \( x \in D_E[0,T] \),
\begin{equation}\label{eq:w-w'-def}
  \begin{split}
    w(x, [c,d))      & \coloneqq \sup_{c \leq s < t < d} d(x_t,x_s),                            \\
    w'(x, \delta) & \coloneqq \inf_{\Set{t_i} \in \cC_T^\delta} \max_i w(x, [t_{i-1}, t_i)).
  \end{split}
\end{equation}

\begin{theorem}[Thm. 10 in~\cite{kouritzinTightnessProbabilityMeasures2015}]\label{prelim:thm:rel-compact-in-SK}
  A set of paths \(A \subseteq D_E[0,T]\) is relatively compact w.r.t.\@ \(d_T^\mathrm{SK}\) if and only if
  \begin{enumerate}[(i)]
    \item there exists a compact set \(K \Subset E\) such that \(x_t \in K\) for all \(t \in [0,T]\), \(x \in A\); and
    \item
          \begin{math}
            \displaystyle \lim_{\delta\to0} \sup_{x \in A} w'(x,\delta) = 0.
          \end{math}
  \end{enumerate}
\end{theorem}

\begin{remark}
  For the space \( D_E[0,\infty) \) a similar characterization of relative compactness exists, which requires the two conditions in \zcref{prelim:thm:rel-compact-in-SK} to hold for all \( T > 0 \); see Theorem 3.6.3 and Remark 3.6.4 in~\cite{ethierMarkovProcessesCharacterization1986}.
\end{remark}

% \begin{theorem}
%   A set of paths \(A \subseteq D_E[0,\infty)\) is relatively compact w.r.t.\@ \(d_\infty^\mathrm{SK}\) if and only if
%   \begin{enumerate}[(i)]
%     \item For all \(T > 0\), there exists a compact set \(\Gamma_T \subset E\) such that \(x_t \in \Gamma_T\) for all \(t \in [0,T]\), \(x \in A\).
%     \item For all \(T > 0\),
%           \begin{equation}
%             \lim_{\delta\to0} \sup_{x \in A} w'(x,\delta,T) = 0.
%           \end{equation}
%   \end{enumerate}
% \end{theorem}
% \begin{proof}
%   This is Theorem 3.6.3 modified based on Remark 3.6.4 from Ethier and Kurz~\cite{ethierMarkovProcessesCharacterization1986}.
% \end{proof}


\subsection{Narrow convergence}

Many different notions of convergence exist for sequences of (probability) measures.
We will focus on \textit{narrow convergence}, which is also often called weak convergence.
On the level of random variables it corresponds to convergence in distribution.

\begin{definition}\label{def:narrow-conv}
  Let \(\Set{\mu_n}_{n\in\bN} \subset \cP(E)\).
  This sequence of probability measures \textit{converges narrowly} to \(\mu \in \cP(E)\), which we denote with \( \mu_n \wto \mu \), if
  \begin{equation}
    \forall \varphi \in C_b(E) : \lim_{n\to\infty} \int_E \varphi \odif{\mu_n} = \int_E \varphi \odif{\mu}.
  \end{equation}
\end{definition}

\begin{remark}
  This is just one possible definition of narrow convergence; there are several alternative, equivalent definitions.
  Together these form the Portmanteau theorem, see~\cite[16]{billingsleyConvergenceProbabilityMeasures1999}.
\end{remark}

Due to the assumption of separability on \( (E, d) \), the topology of narrow convergence is metrizable.
Furthermore, both the separability and completeness of \( (E, d) \) are transferred to this metric space of narrow convergence.

\begin{definition}\label{def:prok-metric}
  The \textit{Prokhorov metric} \( \pi : \cP(E) \times \cP(E) \to [0,1] \) is defined as
  \begin{equation}
    \pi(\mu, \nu) \coloneqq \inf\Set{ \eps > 0 \given \forall A \in \cB(E) : \mu(A) \leq \nu(A^\eps) + \eps }, \quad \mu, \nu \in \cP(E),
  \end{equation}
  where
  \begin{equation}
    A^\eps \coloneqq \Set{ x \in E \given d(x, A) < \eps }.
  \end{equation}
\end{definition}

Note how convergence of \( \mu_n \) to \( \mu \) under this metric does not require \( \mu_n(A) \) and \( \mu(A) \) to get close, as would be required for setwise convergence.
Rather, it allows for an \( \eps \)-margin around the measured set.

\begin{theorem}[Thm. 6.8 in~\cite{billingsleyConvergenceProbabilityMeasures1999}]\label{thm:narrow-metrization}
  The Prokhorov metric from \zcref{def:prok-metric} is indeed a metric, and it metrizes the narrow topology induced by \zcref{def:narrow-conv}.
  The metric space \( (\cP(E), \pi) \) is separable and complete.
\end{theorem}

In \zcref{sec:compactness}, this will allow us to fit the space \( D_{\cP(\potspace)}[0,T] \) into the framework of \zcref{subsec:tight-skorok}, since \zcref{thm:narrow-metrization} implies that \( (\cP(\potspace), \pi) \) is a separable, complete metric space just like \( (E, d) \).

\subsubsection{Tightness}

For a sequence of probability measures to converge narrowly, no \enquote{escape of mass} may occur.
This requirement is formalized through the following concept.

\begin{definition}
  A set of probability measures \( \Pi \subseteq \cP(E) \) is said to be \textit{tight} if
  \begin{equation}
    \forall \eps > 0 : \exists K \Subset E : \forall \mu \in \Pi : \mu(K) > 1 - \eps.
  \end{equation}
\end{definition}

Due to completeness and separability of \((E,d)\), a set consisting of a single probability measure is tight by Theorem 1.3 in~\cite{billingsleyConvergenceProbabilityMeasures1999}.
By extension, any finite \(\Pi \subseteq \cP(E)\) is tight as well (since a finite union of compact sets is compact).

Tightness is not just a necessary condition for convergence: it also implies the existence of a converging subsequence.
This is the subject of Prokhorov's theorem~\cite[57-65]{billingsleyConvergenceProbabilityMeasures1999}.

\begin{theorem}[Prokhorov]\label{thm:prokhorov}
  \( \Pi \subseteq \cP(E) \) is relatively compact if and only if it is tight.
\end{theorem}

There is another equivalent condition to tightness that is sometimes useful.
It is both sufficient and necessary, but for brevity we will only state and prove its sufficiency since that is all we will require.

\begin{lemma}\label{lem:tightness-char-coercive-function}
  Let \( \Pi \subseteq \cP(E) \), and suppose there exists a function \( \phi : E \to [0,\infty] \) with compact sublevel sets such that
  \begin{equation}
    \norm{\phi}_\Pi \coloneqq \sup_{\mu \in \Pi} \int_E \phi \odif{\mu} < \infty.
  \end{equation}
  Then \( \Pi \) is tight.
\end{lemma}

\begin{proof}
  Let \( \eps > 0 \).
  Define \( K \coloneqq \Set{x \in E \given \phi(x) \leq \frac{\norm{\phi}_\Pi}{\eps}} \), which is compact by assumption.
  Then for any \( \mu \in \Pi \),
  \begin{equation}
    \frac{\norm{\phi}_\Pi}{\eps} \mu(E \setminus K) = \int_{E \setminus K} \frac{\norm{\phi}_\Pi}{\eps} \odif{\mu}
    \leq \int_{E \setminus K} \phi \odif{\mu}
    \leq \norm{\phi}_\Pi,
  \end{equation}
  which implies that \( \Pi \) is tight.
  % NOTE: I would like to call phi coercive, but that only means relatively compact I think. Which is maybe sufficient anyways?
\end{proof}


\subsubsection{Tightness in Skorokhod spaces}\label{subsec:tight-skorok}

The goal of \zcref{sec:compactness} will be to apply Prokhorov's theorem to determine the existence of a subsequence in the case where \( \Pi = \Set{\mu_n}_{n\in\bN} \subset \cP(D_{E}[0,T]) \), i.e., a sequence of probability measures on the Skorokhod space consisting of paths in~\(E\).\footnote{In particular, we will consider \(E = \cP(\potspace)\).}
To this end, we will make use of a characterization of tightness specific to probability measures on Skorokhod spaces by Kouritzin~\cite{kouritzinTightnessProbabilityMeasures2015}.

There, the results are provided on the level of processes, but we will instead formulate them for the laws of those processes (which is entirely equivalent).
Kouritzin's setting is also very general, with \( E \) being a completely regular topological space.
We will restrict Kouritzin's results to our context, in which \((E,d)\) is a Polish metric space.

\begin{definition}[CCC]\label{def:CCC}
  Let \( \Pi \subseteq \cP(D_{E}[0,T]) \).
  Then \( \Pi \) satisfies the \textit{compact containment condition} if
  \begin{equation}
    \forall \eps > 0 : \exists K \Subset E : \inf_{\mu \in \Pi} \mu(\Set{x \in D_E[0,T] \given \forall t \in [0,T] : x_t \in K}) \geq 1 - \eps.
  \end{equation}
\end{definition}

\begin{definition}[MCC]\label{def:MCC}
  Let \( \Pi \subseteq \cP(D_{E}[0,T]) \).
  Then \( \Pi \) satisfies the \textit{modulus of continuity condition} if
  \begin{equation}\label{eq:MCC}
    \forall \eta > 0 : \exists \delta > 0 : \sup_{\mu \in \Pi} \mu(\Set{x \in D_E[0,T] \given w'(x,\delta) \geq \eta}) \leq \eta.
  \end{equation}
  It satisfies the \textit{asymptotic modulus of continuity condition} (AMCC) if for every \( \eta > 0 \) there exists a \( \delta > 0 \) and a finite set \( \Pi_0 \subset \Pi \) such that the statement in \zcref{eq:MCC} holds with \( \Pi \) replaced by \( \Pi \setminus \Pi_0 \).
\end{definition}

Note how these two conditions relate to the two conditions in \zcref{prelim:thm:rel-compact-in-SK}.
The following result is therefore not surprising.

\begin{theorem}[Thm. 13 in~\cite{kouritzinTightnessProbabilityMeasures2015}]
  Suppose \( \Pi \subseteq \cP(D_E[0,T]) \) satisfies both the CCC and the MCC.\@ Then \( \Pi \) is tight.
\end{theorem}

\begin{remark}
  This characterization of tightness can be adapted to \( D_E[0,\infty) \) in a straightforward manner; see Theorem 3.7.2 and Remark 3.7.3 from~\cite{ethierMarkovProcessesCharacterization1986}.
\end{remark}

However, verifying the CCC and/or the MCC can be difficult.
To that end, Kouritzin introduces weaker alternative conditions.
One can replace either the CCC or the MCC (not both) with such a weaker condition, and still preserve the tightness result.
We will only require the substitution of the MCC by a weaker alternative, which we will state after defining a separation property for collections of functions.

\begin{definition}\label{def:s.p.}
  A set \( \cG \subseteq M(E) \) \textit{separates points} if for every pair of distinct points \( x, y \in E \) there exists a \( g \in \cG \) such that \( g(x) \neq g(y) \).
\end{definition}

\begin{definition}[WMCC]\label{def:WMCC}
  Let \( \Pi \subseteq \cP(D_{E}[0,T]) \).
  Then \( \Pi \) satisfies the \textit{weak modulus of continuity condition} if there exists a \( \cG \subseteq C(E) \) that separates points,\footnote{Kouritzin only requires separation of points to hold on compact sets, but this is equivalent to global separation of points (since any set \( \Set{x,y} \subseteq E \) is compact (in any topological space)).}
  % NOTE So the weakening to "on compacts" added by Kouritzin seems to be meaningless?
  and the set of pushforward measures \( \Set{g \sharp \mu \given \mu \in \Pi} \subseteq \cP(D_\bR[0,T]) \)\footnote{We abuse notation to identify \( g : E \to \bR \) with the map \( D_E[0,T] \mapsto D_\bR[0,T] \) that applies \( g \) pointwise.} satisfies the AMCC for all \( g \in \cG \cup \Set{f + h \given f, h \in \cG} \).
  (The metric on \( \bR \) is induced by the usual Euclidean norm \( \abs{\cdot} \).)
  % and \( \Set{g \circ X^\alpha} \) satisfies the AMCC on \( (\bR, \abs{\cdot}) \) for all \( g \in \cG \cup \Set{f + h \given f, h \in \cG} \).
\end{definition}

By Theorem 20 from Kourizin \cite{kouritzinTightnessProbabilityMeasures2015}, we can still achieve tightness with this weaker condition.

\begin{theorem}\label{thm:ccc+wmcc=tight}
  If \( \Pi \subseteq \cP(D_{E}[0,T]) \) satisfies the CCC and the WMCC, then it is tight.
\end{theorem}
% NOTE: Theorem 20 in Kouritzin is (again) formulated for processes, and it yields tightness only for indistinguishable processes. But indistinguishable processes have the same law, so I don't understand why that is different from just X^alpha being tight?

\subsection{(Semi)martingales}

In \zcref{sec:} we will use various concepts and tools from stochastic calculus, which we will briefly enumerate here.
A more detailed explanation can be found in~\cite{klebanerIntroductionStochasticCalculus2012}.

\begin{definition}
  A stochastic process \(\Set{M_t}_{t}\) is a \textit{martingale} if it is adapted to a filtration \(\Set{\cF_t}_t\), \( M_t \) is integrable for every \(t\), and for each \( s < t \) it satisfies
  \begin{equation}
    \Exp{}{M_t \given \cF_s} = M_s \quad \text{a.s.}
  \end{equation}
  It is a \textit{local martingale} if there exists an a.s.\ increasing sequence of stopping times \( \tau_n \) with \( \tau_n \uparrow \infty \) a.s.\ such that the stopped process \( M_{t \land \tau_n} \) is a martingale for all \( n \in \bN \). % NOTE: Klebaner 2012 requires uniform integrability, but by Proposition 4.4 in the Stochastic Integration lecture notes this is equivalent
\end{definition}

Of course, a martingale is also a local martingale.
This follows by choosing \( \tau_n \coloneqq n \), and using that a stopped martingale is still a martingale.

\begin{definition}
  The \textit{variation} of a real-valued function \( g \) that is defined on an interval \( [a,b] \) is equal to
  \begin{equation}
    V_g([a,b]) \coloneqq \sup \sum_{i=1}^n \abs{ g(t_i^n) - g(t_{i-1}) }
  \end{equation}
  where we take the supremum over all possible partitions \(\Set{(t_i^n)_{i=1,\dots,n}}_{n\in\bN}\) of the interval \( [a,b] \).
  The function \( g \) is of \textit{finite variation} if its variation over any bounded interval is finite.
  % It is of \textit{bounded variation} if \( \sup_t V_g([0,t]) < \infty \).    % Only define this if needed (otherwise confusing)
\end{definition}

\begin{definition}\label{def:semimart}
  A stochastic process \(\Set{S_t}_{t}\) with càdlàg sample paths adapted to a filtration \(\Set{\cF_t}_t\) is called a \textit{semimartingale} if it can be decomposed as
  \begin{equation}
    S_t = S_0 + M_t + A_t,
  \end{equation}
  where \( \Set{M_t}_{t} \) is a local martingale w.r.t. \(\Set{\cF_t}_t\) and \( \Set{A_t}_{t} \) is a process whose paths are a.s. of finite variation, and \( M_0 = A_0 = 0 \) a.s.
\end{definition}

\begin{definition}
  If it exists, the \textit{quadratic variation} of a stochastic process \(\Set{X_t}_{t}\) at time \( t \) equals
  \begin{equation}
    [X]_t \coloneqq \lim \sum_{i=1}^n (X_{t_i}^n - X_{t_{i-1}}^n)^2,
  \end{equation}
  where the limit is a limit in probability over shrinking partitions \(\Set{(t_i^n)_{i=1,\dots,n}}_{n\in\bN}\) of the interval \( [0,t] \).
\end{definition}

For the type of stochastic process we will consider we will not have to worry about the existence of the quadratic variation, due to the following result.

\begin{lemma}[p.220 in~\cite{klebanerIntroductionStochasticCalculus2012}]
  If \(\Set{S_t}_{t}\) is a semimartingale according to \zcref{def:semimart}, then its quadratic variation \( [S]_t \) exists for all \( t \).
\end{lemma}


\subsection{The martingale problem on \texorpdfstring{\(D_E[0,\infty)\)}{D([0,infinity), E)}}\label{sec:martingale-problem}

Problems involving stochastic dynamics are often formulated as a \textit{martingale problem}.
For our definition of the martingale problem, we will follow Ethier and Kurz~\cite[p.174]{ethierMarkovProcessesCharacterization1986}.
Let \((A, D(A)) : M_b(E) \to M_b(E)\) be a linear operator.\footnote{The definition can be extended to multivalued, nonlinear operators, but we will not require such generality.}
We say that a probability measure \(P \in \cP(D_E[0,\infty))\) is a solution of the martingale problem for \( A \) if for the canonical coordinate process
\begin{equation}
  X_t(\omega) \coloneqq \omega_t, \quad \omega \in D_E[0,\infty), t \geq 0,
\end{equation}
defined on the probability space \((D_E[0,\infty), \mathscr{S}_E, P)\), it holds that for all test functions \(f \in D(A)\)
\begin{equation}\label{eq:mart-problem}
  t \mapsto f(X_t) - f(X_0) - \int_0^t (Af)(X_s) \odif{s}
\end{equation}
is a martingale with respect to the filtration\footnote{In general the filtration is more complicated, but as \( X \) is right-continuous it simplifies to this form~\cite[bottom of p. 173]{ethierMarkovProcessesCharacterization1986}.} \(\Set{\cF_t^X}_{t\geq0} \coloneqq \Set{\sigma(X_s : s \leq t)}\). % equals \mathscr{S}_E restricted to [0,t], see citation of Prop. 3.7.1 above
This eponymous condition is what makes the problem relevant in the study of stochastic processes.
It is satisfied whenever \(A\) is the generator of a Markov process \(X\)~\cite[161-162]{ethierMarkovProcessesCharacterization1986}.

If additionally \( X_0 \sharp P = P X_0^{-1} = \mu_0 \) for a prescribed initial distribution \(\mu_0 \in \cP(E)\), we say that \(P\) is a solution of the martingale problem for \((A,\mu_0)\).
Such a solution is called unique if for any two solutions the finite-dimensional distributions (of \(X\)) are identical.
The martingale problem for \((A,\mu_0)\) is well-posed if (i) a solution exists and (ii) it is unique.
If this holds for all \(\mu_0 \in \cP(E)\), we say that the martingale problem for \(A\) is well-posed.

Also, because the canonical coordinate process is càdlàg it has at most countably many discontinuities~\cite[Lemma 3.5.1]{ethierMarkovProcessesCharacterization1986}.
% See https://math.stackexchange.com/questions/441721/the-set-of-jumps-of-a-c%C3%A0dl%C3%A0g-function-is-countable
As any countable set is a Lebesgue null set, this implies that
\begin{equation}\label{eq:mart-problem-s-}
  t \mapsto f(X_t) - f(X_0) - \int_0^t (Af)(X_s) \odif{s}
  = f(X_t) - f(X_0) - \int_0^t (Af)(X_{s-}) \odif{s},
\end{equation}
which is thus an equivalent formulation of the martingale problem.
