
@book{pollard_convergence_1984,
	location = {New York, {NY}},
	title = {Convergence of Stochastic Processes},
	rights = {http://www.springer.com/tdm},
	isbn = {978-1-4612-9758-1 978-1-4612-5254-2},
	url = {http://link.springer.com/10.1007/978-1-4612-5254-2},
	series = {Springer Series in Statistics},
	publisher = {Springer New York},
	author = {Pollard, David},
	urldate = {2025-02-04},
	date = {1984},
	langid = {english},
	doi = {10.1007/978-1-4612-5254-2},
}

@book{ethier_markov_1985,
	title = {Markov Processes: Characterization and Convergence},
	author = {Ethier, Stewart N and Kurtz, Thomas G},
	date = {1985},
	langid = {english},
}

@misc{dempster_martingale_1998,
	title = {On the Martingale Problem for Jumping},
	url = {https://www.researchgate.net/publication/312942197_On_the_martingale_problem_for_jumping_diffusions},
	abstract = {Jumping diffusion models for financial prices and returns are finding increasing application in the pricing of current contingent claims. Generalizing the theory for a Markov process with continuous sample paths — characterized in terms of its infinitesimal generator — we establish existence and uniqueness of solutions to jump stochastic differential equations. We adapt the Stroock and Varadhan approach, which develops a variant of the ‘weak sense’ solution of a stochastic differential equation by formulating it as a diffusion process solution of a martingale problem. Our approach to deriving the integro-partial differential equation for the value of a contingent claim through the corresponding Kolmogorov forward equation is illustrated by a generalization of the recent work of Babbs and Webber describing fixed income derivative valuation in the presence of central bank rate changes.},
	publisher = {{ResearchGate}},
	author = {Dempster, Michael A.H. and Gotsis, G.Ch.},
	urldate = {2025-01-28},
	date = {1998-05},
	langid = {english},
}

@misc{luo_phase_2020,
	title = {Phase diagram for two-layer {ReLU} neural networks at infinite-width limit},
	url = {http://arxiv.org/abs/2007.07497},
	doi = {10.48550/arXiv.2007.07497},
	abstract = {How neural network behaves during the training over different choices of hyperparameters is an important question in the study of neural networks. In this work, inspired by the phase diagram in statistical mechanics, we draw the phase diagram for the two-layer {ReLU} neural network at the infinite-width limit for a complete characterization of its dynamical regimes and their dependence on hyperparameters related to initialization. Through both experimental and theoretical approaches, we identify three regimes in the phase diagram, i.e., linear regime, critical regime and condensed regime, based on the relative change of input weights as the width approaches infinity, which tends to \$0\$, \$O(1)\$ and \$+{\textbackslash}infty\$, respectively. In the linear regime, {NN} training dynamics is approximately linear similar to a random feature model with an exponential loss decay. In the condensed regime, we demonstrate through experiments that active neurons are condensed at several discrete orientations. The critical regime serves as the boundary between above two regimes, which exhibits an intermediate nonlinear behavior with the mean-field model as a typical example. Overall, our phase diagram for the two-layer {ReLU} {NN} serves as a map for the future studies and is a first step towards a more systematical investigation of the training behavior and the implicit regularization of {NNs} of different structures.},
	number = {{arXiv}:2007.07497},
	publisher = {{arXiv}},
	author = {Luo, Tao and Xu, Zhi-Qin John and Ma, Zheng and Zhang, Yaoyu},
	urldate = {2025-01-17},
	date = {2020-10-13},
	eprinttype = {arxiv},
	eprint = {2007.07497 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{pesme_saddle--saddle_2023,
	title = {Saddle-to-Saddle Dynamics in Diagonal Linear Networks},
	url = {http://arxiv.org/abs/2304.00488},
	doi = {10.48550/arXiv.2304.00488},
	abstract = {In this paper we fully describe the trajectory of gradient flow over 2-layer diagonal linear networks for the regression setting in the limit of vanishing initialisation. We show that the limiting flow successively jumps from a saddle of the training loss to another until reaching the minimum ℓ1-norm solution. We explicitly characterise the visited saddles as well as the jump times through a recursive algorithm reminiscent of the {LARS} algorithm used for computing the Lasso path. Starting from the zero vector, coordinates are successively activated until the minimum ℓ1-norm solution is recovered, revealing an incremental learning. Our proof leverages a convenient arc-length time-reparametrisation which enables to keep track of the transitions between the jumps. Our analysis requires negligible assumptions on the data, applies to both under and overparametrised settings and covers complex cases where there is no monotonicity of the number of active coordinates. We provide numerical experiments to support our findings.},
	number = {{arXiv}:2304.00488},
	publisher = {{arXiv}},
	author = {Pesme, Scott and Flammarion, Nicolas},
	urldate = {2025-01-17},
	date = {2023-10-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2304.00488 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{boursier_simplicity_2024,
	title = {Simplicity bias and optimization threshold in two-layer {ReLU} networks},
	url = {http://arxiv.org/abs/2410.02348},
	doi = {10.48550/arXiv.2410.02348},
	abstract = {Understanding generalization of overparametrized neural networks remains a fundamental challenge in machine learning. Most of the literature mostly studies generalization from an interpolation point of view, taking convergence of parameters towards a global minimum of the training loss for granted. While overparametrized architectures indeed interpolated the data for typical classification tasks, this interpolation paradigm does not seem valid anymore for more complex tasks such as in-context learning or diffusion. Instead for such tasks, it has been empirically observed that the trained models goes from global minima to spurious local minima of the training loss as the number of training samples becomes larger than some level we call optimization threshold. While the former yields a poor generalization to the true population loss, the latter was observed to actually correspond to the minimiser of this true loss. This paper explores theoretically this phenomenon in the context of two-layer {ReLU} networks. We demonstrate that, despite overparametrization, networks often converge toward simpler solutions rather than interpolating the training data, which can lead to a drastic improvement on the test loss with respect to interpolating solutions. Our analysis relies on the so called early alignment phase, during which neurons align towards specific directions. This directional alignment, which occurs in the early stage of training, leads to a simplicity bias, wherein the network approximates the ground truth model without converging to the global minimum of the training loss. Our results suggest that this bias, resulting in an optimization threshold from which interpolation is not reached anymore, is beneficial and enhances the generalization of trained models.},
	number = {{arXiv}:2410.02348},
	publisher = {{arXiv}},
	author = {Boursier, Etienne and Flammarion, Nicolas},
	urldate = {2025-01-17},
	date = {2024-10-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2410.02348 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{boursier_early_2024,
	title = {Early alignment in two-layer networks training is a two-edged sword},
	url = {http://arxiv.org/abs/2401.10791},
	doi = {10.48550/arXiv.2401.10791},
	abstract = {Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al. (2018). For small initialisation and one hidden {ReLU} layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and only converge to a spurious stationary point instead.},
	number = {{arXiv}:2401.10791},
	publisher = {{arXiv}},
	author = {Boursier, Etienne and Flammarion, Nicolas},
	urldate = {2025-01-17},
	date = {2024-09-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2401.10791 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{crisan_equivalence_2011,
	location = {Berlin, Heidelberg},
	title = {Equivalence of Stochastic Equations and Martingale Problems},
	isbn = {978-3-642-15357-0 978-3-642-15358-7},
	url = {https://link.springer.com/10.1007/978-3-642-15358-7_6},
	abstract = {The fact that the solution of a martingale problem for a diﬀusion process gives a weak solution of the corresponding Itˆo equation is well-known since the original work of Stroock and Varadhan. The result is typically proved by constructing the driving Brownian motion from the solution of the martingale problem and perhaps an auxiliary Brownian motion. This constructive approach is much more challenging for more general Markov processes where one would be required to construct a Poisson random measure from the sample paths of the solution of the martingale problem. A “soft” approach to this equivalence is presented here which begins with a joint martingale problem for the solution of the desired stochastic equation and the driving processes and applies a Markov mapping theorem to show that any solution of the original martingale problem corresponds to a solution of the joint martingale problem. These results coupled with earlier results on the equivalence of forward equations and martingale problems show that the three standard approaches to specifying Markov processes (stochastic equations, martingale problems, and forward equations) are, under very general conditions, equivalent in the sense that existence and/or uniqueness of one implies existence and/or uniqueness for the other two.},
	pages = {113--130},
	booktitle = {Stochastic Analysis 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Kurtz, Thomas G.},
	editor = {Crisan, Dan},
	urldate = {2024-12-31},
	date = {2011},
	langid = {english},
	doi = {10.1007/978-3-642-15358-7_6},
}

@book{protter_stochastic_1990,
	location = {Berlin, Heidelberg},
	title = {Stochastic Integration and Differential Equations},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-662-02621-2 978-3-662-02619-9},
	url = {http://link.springer.com/10.1007/978-3-662-02619-9},
	publisher = {Springer Berlin Heidelberg},
	author = {Protter, Philip},
	urldate = {2024-12-31},
	date = {1990},
	langid = {english},
	doi = {10.1007/978-3-662-02619-9},
}

@article{caceres_beyond_2014,
	title = {Beyond blow-up in excitatory integrate and fire neuronal networks: Refractory period and spontaneous activity},
	volume = {350},
	issn = {00225193},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022519314000666},
	doi = {10.1016/j.jtbi.2014.02.005},
	shorttitle = {Beyond blow-up in excitatory integrate and fire neuronal networks},
	abstract = {The Network Noisy Leaky Integrate and Fire equation is among the simplest model allowing for a selfconsistent description of neural networks and gives a rule to determine the probability to ﬁnd a neuron at the potential v. However, its mathematical structure is still poorly understood and, concerning its solutions, very few results are available. In the midst of them, a recent result shows blow-up in ﬁnite time for fully excitatory networks. The intuitive explanation is that each ﬁring neuron induces a discharge of the others; thus increases the activity and consequently the discharge rate of the full network.},
	pages = {81--89},
	journaltitle = {Journal of Theoretical Biology},
	shortjournal = {Journal of Theoretical Biology},
	author = {Cáceres, María J. and Perthame, Benoît},
	urldate = {2024-12-31},
	date = {2014-06},
	langid = {english},
}

@book{conway_course_2007,
	location = {New York, {NY}},
	title = {A Course in Functional Analysis},
	volume = {96},
	rights = {http://www.springer.com/tdm},
	isbn = {978-1-4419-3092-7 978-1-4757-4383-8},
	url = {http://link.springer.com/10.1007/978-1-4757-4383-8},
	series = {Graduate Texts in Mathematics},
	publisher = {Springer New York},
	author = {Conway, John B.},
	urldate = {2024-12-18},
	date = {2007},
	langid = {english},
	doi = {10.1007/978-1-4757-4383-8},
}

@article{jakubowski_skorokhod_nodate,
	title = {On the Skorokhod topology},
	abstract = {Let E be a completely regular topological space. Mitoma [9 ], extending the classical case E = R 1, has recently introduced the Skorokhod topology on the space D( [0, 1 ] : E). This topology is investigated in detail. We find families of continuous functions which generate the topology, examine the structure of the Borel and Baire a-algebras of D( [0, 1 ] : E) and prove tightness criteria for E-valued stochastic processes. Extensions to D(R + : E) are also given.},
	author = {Jakubowski, Adam},
	langid = {english},
}

@article{kouritzin_tightness_2015,
	title = {On tightness of probability measures on Skorokhod spaces},
	volume = {368},
	rights = {https://www.ams.org/publications/copyright-and-permissions},
	issn = {0002-9947, 1088-6850},
	url = {https://www.ams.org/tran/2016-368-08/S0002-9947-2015-06522-2/},
	doi = {10.1090/tran/6522},
	abstract = {The equivalences to and the connections between the modulusof-continuity condition, compact containment and tightness on {DE} [a, b] with a {\textless} b are studied. The results within are tools for establishing tightness for probability measures on {DE} [a, b] that generalize and simplify prevailing results in the cases that E is a metric space, nuclear space dual or, more generally, a completely regular topological space. Applications include establishing weak convergence to martingale problems, the long-time typical behavior of nonlinear ﬁlters and particle approximation of cadlag probability-measure-valued processes. This particle approximation is studied herein, where the distribution of the particles is the underlying measure-valued process at an arbitrarily ﬁne discrete mesh of points.},
	pages = {5675--5700},
	number = {8},
	journaltitle = {Transactions of the American Mathematical Society},
	shortjournal = {Trans. Amer. Math. Soc.},
	author = {Kouritzin, Michael},
	urldate = {2024-12-17},
	date = {2015-11-16},
	langid = {english},
}

@book{klebaner_introduction_2012,
	location = {London},
	edition = {Third edition},
	title = {Introduction to stochastic calculus with applications},
	isbn = {978-1-911298-76-0},
	pagetotal = {438},
	publisher = {{ICP}, Imperial College Press},
	author = {Klebaner, Fima C.},
	date = {2012},
	langid = {english},
}

@article{champagnat_unifying_2006,
	title = {Unifying evolutionary dynamics: From individual stochastic processes to macroscopic models},
	volume = {69},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00405809},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0040580905001632},
	doi = {10.1016/j.tpb.2005.10.004},
	shorttitle = {Unifying evolutionary dynamics},
	pages = {297--321},
	number = {3},
	journaltitle = {Theoretical Population Biology},
	shortjournal = {Theoretical Population Biology},
	author = {Champagnat, Nicolas and Ferrière, Régis and Méléard, Sylvie},
	urldate = {2024-10-24},
	date = {2006-05},
	langid = {english},
}

@article{fournier_microscopic_2004,
	title = {A microscopic probabilistic description of a locally regulated population and macroscopic approximations},
	volume = {14},
	issn = {1050-5164},
	url = {https://projecteuclid.org/journals/annals-of-applied-probability/volume-14/issue-4/A-microscopic-probabilistic-description-of-a-locally-regulated-population-and/10.1214/105051604000000882.full},
	doi = {10.1214/105051604000000882},
	number = {4},
	journaltitle = {The Annals of Applied Probability},
	shortjournal = {Ann. Appl. Probab.},
	author = {Fournier, Nicolas and Méléard, Sylvie},
	urldate = {2024-10-24},
	date = {2004-11-01},
	langid = {english},
}

@book{whitt_stochastic-process_2002,
	location = {New York, {NY}},
	title = {Stochastic-Process Limits: An Introduction to Stochastic-Process Limits and Their Application to Queues},
	isbn = {978-0-387-95358-8 978-0-387-21748-2},
	series = {Springer Series in Operations Research and Financial Engineering Ser},
	shorttitle = {Stochastic-Process Limits},
	pagetotal = {1},
	publisher = {Springer New York},
	author = {Whitt, Ward},
	date = {2002},
	langid = {english},
}

@book{billingsley_convergence_1999,
	location = {New York Weinheim},
	edition = {2. ed},
	title = {Convergence of probability measures},
	isbn = {978-0-471-19745-4 978-0-470-31780-8},
	series = {Wiley series in probability and statistics Probability and statistics section},
	pagetotal = {277},
	publisher = {Wiley},
	author = {Billingsley, Patrick},
	date = {1999},
	langid = {english},
}

@article{kern_skorokhod_2024,
	title = {The Skorokhod Topologies: What They Are and Why We Should Care},
	volume = {71},
	issn = {0720-728X, 1432-1815},
	url = {http://arxiv.org/abs/2210.16026},
	doi = {10.1007/s00591-023-00353-2},
	shorttitle = {The Skorokhod Topologies},
	abstract = {This paper presents a gentle and informal introduction to the Skorokhod topologies. Focus is on motivating examples and concepts.},
	pages = {1--18},
	number = {1},
	journaltitle = {Mathematische Semesterberichte},
	shortjournal = {Math Semesterber},
	author = {Kern, Julian},
	urldate = {2024-10-17},
	date = {2024-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2210.16026 [math]},
	keywords = {Mathematics - General Topology, Mathematics - Probability},
}

@misc{gao_optimal_2023,
	title = {Optimal control formulation of transition path problems for Markov Jump Processes},
	url = {http://arxiv.org/abs/2311.07795},
	abstract = {Among various rare events, the eﬀective computation of transition paths connecting metastable states in a stochastic model is an important problem. This paper proposes a stochastic optimal control formulation for transition path problems in an inﬁnite time horizon for Markov jump processes on polish space. An unbounded terminal cost at a stopping time and a controlled transition rate for the jump process regulate the transition from one metastable state to another. The running cost is taken as an entropy form of the control velocity, in contrast to the quadratic form for diﬀusion processes. Using the Girsanov transformation for Markov jump processes, the optimal control problem in both ﬁnite time and inﬁnite time horizon with stopping time ﬁt into one framework: the optimal change of measures in the C`adla`g path space via minimizing their relative entropy. We prove that the committor function, solved from the backward equation with appropriate boundary conditions, yields an explicit formula for the optimal path measure and the associated optimal control for the transition path problem. The unbounded terminal cost leads to a singular transition rate (unbounded control velocity), for which, the Gamma convergence technique is applied to pass the limit for a regularized optimal path measure. The limiting path measure is proved to solve a Martingale problem with an optimally controlled transition rate and the associated optimal control is given by Doob-h transformation. The resulting optimally controlled process can realize the transitions almost surely.},
	number = {{arXiv}:2311.07795},
	publisher = {{arXiv}},
	author = {Gao, Yuan and Liu, Jian-Guo and Tse, Oliver},
	urldate = {2024-10-10},
	date = {2023-11-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2311.07795 [math]},
	keywords = {Mathematics - Analysis of {PDEs}, Mathematics - Optimization and Control, Mathematics - Probability},
}

@article{delarue_global_2015,
	title = {Global solvability of a networked integrate-and-fire model of {McKean}-Vlasov type},
	volume = {25},
	issn = {1050-5164},
	url = {http://arxiv.org/abs/1211.0299},
	doi = {10.1214/14-AAP1044},
	abstract = {We here investigate the well-posedness of a networked integrate-and-fire model describing an infinite population of neurons which interact with one another through their common statistical distribution. The interaction is of the self-excitatory type as, at any time, the potential of a neuron increases when some of the others fire: precisely, the kick it receives is proportional to the instantaneous proportion of firing neurons at the same time. From a mathematical point of view, the coefficient of proportionality, denoted by \${\textbackslash}alpha\$, is of great importance as the resulting system is known to blow-up for large values of \${\textbackslash}alpha\$. In the current paper, we focus on the complementary regime and prove that existence and uniqueness hold for all time when \${\textbackslash}alpha\$ is small enough.},
	number = {4},
	journaltitle = {The Annals of Applied Probability},
	shortjournal = {Ann. Appl. Probab.},
	author = {Delarue, François and Inglis, James and Rubenthaler, Sylvain and Tanré, Etienne},
	urldate = {2024-09-26},
	date = {2015-08-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1211.0299 [math]},
	keywords = {Mathematics - Probability},
}

@misc{delarue_particle_2015,
	title = {Particle systems with a singular mean-field self-excitation. Application to neuronal networks},
	url = {http://arxiv.org/abs/1406.1151},
	abstract = {We discuss the construction and approximation of solutions to a nonlinear {McKean}-Vlasov equation driven by a singular self-excitatory interaction of the meanﬁeld type. Such an equation is intended to describe an inﬁnite population of neurons which interact with one another. Each time a proportion of neurons ‘spike’, the whole network instantaneously receives an excitatory kick. The instantaneous nature of the excitation makes the system singular and prevents the application of standard results from the literature. Making use of the Skorohod M1 topology, we prove that, for the right notion of a ‘physical’ solution, the nonlinear equation can be approximated either by a ﬁnite particle system or by a delayed equation. As a by-product, we obtain the existence of ‘synchronized’ solutions, for which a macroscopic proportion of neurons may spike at the same time.},
	number = {{arXiv}:1406.1151},
	publisher = {{arXiv}},
	author = {Delarue, F. and Inglis, J. and Rubenthaler, S. and Tanré, E.},
	urldate = {2024-09-26},
	date = {2015-01-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1406.1151 [math]},
	keywords = {Mathematics - Probability},
}

@thesis{ambrogi_pdes_nodate,
	title = {{PDEs} for neural networks with internal states},
	type = {phdthesis},
	author = {Ambrogi, Elena},
	langid = {english},
}

@article{liu_investigating_2021,
	title = {Investigating the integrate and fire model as the limit of a random discharge model: a stochastic analysis perspective},
	volume = {Volume 1},
	rights = {https://arxiv.org/licenses/nonexclusive-distrib/1.0},
	issn = {2801-0159},
	url = {https://mna.episciences.org/7203},
	doi = {10.46298/mna.7203},
	shorttitle = {Investigating the integrate and fire model as the limit of a random discharge model},
	abstract = {In the mean ﬁeld integrate-and-ﬁre model, the dynamics of a typical neuron within a large network is modeled as a diffusion-jump stochastic process whose jump takes place once the voltage reaches a threshold. In this work the main goal is to establish the convergence relationship between a regularized process and the original one where in the regularized process, the jump mechanism is replaced by a Poisson dynamic, and jump intensity within the classically forbidden domain goes to inﬁnity as the regularization parameter vanishes. On the macroscopic level, the Fokker-Planck equation for the process with random discharges (i.e. Poisson jumps) is deﬁned on the whole space, while the equation for the limit process is deﬁned on the half space. However, using an iteration scheme the difﬁculty due to the domain differences has been greatly mitigated and the convergence for the stochastic process and the ﬁring rates can be established. Moreover, we ﬁnd polynomial-order convergence for the distribution by a re-normalization argument in probability theory. Finally, using numerical experiments we quantitatively explore the rate and the asymptotic behavior of convergence for both linear and nonlinear models.},
	pages = {7203},
	journaltitle = {Mathematical Neuroscience and Applications},
	author = {Liu, Jian-Guo and Wang, Ziheng and Xie, Yantong and Zhang, Yuan and Zhou, Zhennan},
	urldate = {2024-09-12},
	date = {2021-11-30},
	langid = {english},
}

@article{chaintron_propagation_2022,
	title = {Propagation of chaos: a review of models, methods and applications. {II}. Applications},
	volume = {15},
	issn = {1937-5093, 1937-5077},
	url = {http://arxiv.org/abs/2106.14812},
	doi = {10.3934/krm.2022018},
	shorttitle = {Propagation of chaos},
	abstract = {The notion of propagation of chaos for large systems of interacting particles originates in statistical physics and has recently become a central notion in many areas of applied mathematics. The present review describes old and new methods as well as several important results in the ﬁeld. The models considered include the {McKean}-Vlasov diﬀusion, the mean-ﬁeld jump models and the Boltzmann models. The ﬁrst part of this review is an introduction to modelling aspects of stochastic particle systems and to the notion of propagation of chaos. The second part presents concrete applications and a more detailed study of some of the important models in the ﬁeld.},
	pages = {1017},
	number = {6},
	journaltitle = {Kinetic and Related Models},
	shortjournal = {{KRM}},
	author = {Chaintron, Louis-Pierre and Diez, Antoine},
	urldate = {2024-09-12},
	date = {2022},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2106.14812 [math-ph]},
	keywords = {82C22, 82C40, 35Q70, 65C35, 92-10, Mathematical Physics, Mathematics - Analysis of {PDEs}, Mathematics - History and Overview, Mathematics - Probability},
}

@article{chaintron_propagation_2022-1,
	title = {Propagation of chaos: a review of models, methods and applications. I. Models and methods},
	volume = {15},
	issn = {1937-5093, 1937-5077},
	url = {http://arxiv.org/abs/2203.00446},
	doi = {10.3934/krm.2022017},
	shorttitle = {Propagation of chaos},
	abstract = {The notion of propagation of chaos for large systems of interacting particles originates in statistical physics and has recently become a central notion in many areas of applied mathematics. The present review describes old and new methods as well as several important results in the ﬁeld. The models considered include the {McKean}-Vlasov diﬀusion, the mean-ﬁeld jump models and the Boltzmann models. The ﬁrst part of this review is an introduction to modelling aspects of stochastic particle systems and to the notion of propagation of chaos. The second part presents concrete applications and a more detailed study of some of the important models in the ﬁeld.},
	pages = {895},
	number = {6},
	journaltitle = {Kinetic and Related Models},
	shortjournal = {{KRM}},
	author = {Chaintron, Louis-Pierre and Diez, Antoine},
	urldate = {2024-09-12},
	date = {2022},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.00446 [math-ph]},
	keywords = {82C22, 82C40, 35Q70, 65C35, 92-10, Mathematical Physics, Mathematics - Analysis of {PDEs}, Mathematics - History and Overview, Mathematics - Probability},
}

@misc{caceres_analysis_2010,
	title = {Analysis of Nonlinear Noisy Integrate{\textbackslash}\&Fire Neuron Models: blow-up and steady states},
	url = {http://arxiv.org/abs/1010.4683},
	shorttitle = {Analysis of Nonlinear Noisy Integrate{\textbackslash}\&Fire Neuron Models},
	abstract = {Nonlinear Noisy Leaky Integrate and Fire ({NNLIF}) models for neurons networks can be written as Fokker-Planck-Kolmogorov equations on the probability density of neurons, the main parameters in the model being the connectivity of the network and the noise. We analyse several aspects of the {NNLIF} model: the number of steady states, a priori estimates, blow-up issues and convergence toward equilibrium in the linear case. In particular, for excitatory networks, blow-up always occurs for initial data concentrated close to the ﬁring potential. These results show how critical is the balance between noise and excitatory/inhibitory interactions to the connectivity parameter.},
	number = {{arXiv}:1010.4683},
	publisher = {{arXiv}},
	author = {Cáceres, María J. and Carrillo, José A. and Perthame, Benoît},
	urldate = {2024-09-12},
	date = {2010-10-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1010.4683 [math, q-bio]},
	keywords = {35K60, 82C31, 92B20, Mathematics - Analysis of {PDEs}, Quantitative Biology - Neurons and Cognition},
}

@misc{faugeras_universality_2024,
	title = {Universality of the mean-field equations of networks of Hopfield-like neurons},
	url = {http://arxiv.org/abs/2408.14290},
	abstract = {We revisit the problem of characterising the mean-field limit of a network of Hopfield-like neurons. Building on the previous work of [13, 1, 14] and [9] we establish for a large class of networks of Hopfield-like neurons, i.e. rate neurons, the mean-field equations on a time interval [0, T ], T {\textgreater} 0, of the thermodynamic limit of these networks, i.e. the limit when the number of neurons goes to infinity. Unlike all previous work, except [9], we do not assume that the synaptic weights describing the connections between the neurons are i.i.d. as zero-mean Gaussians. The limit equations are stochastic and very simply described in terms of two functions, a “correlation” function noted {KQ}(t, s) and a “mean” function noted {mQ}(t). The “noise” part of the equations is a linear function of the Brownian motion, which is obtained by solving a Volterra equation of the second kind whose resolving kernel is expressed as a function of {KQ}. We give a constructive proof of the uniqueness of the limit equations. We use the corresponding algorithm for an effective computation of the functions {KQ} and {mQ}, given the weights distribution. Several numerical experiments are reported.},
	number = {{arXiv}:2408.14290},
	publisher = {{arXiv}},
	author = {Faugeras, Olivier and Tanré, Etienne},
	urldate = {2024-09-08},
	date = {2024-08-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2408.14290 [math]},
	keywords = {60F10, 92B20, 60B10, Mathematics - Probability},
}

@article{grazieschi_network_2019,
	title = {Network of interacting neurons with random synaptic weights},
	volume = {65},
	rights = {http://creativecommons.org/licenses/by/4.0},
	issn = {2267-3059},
	url = {https://www.esaim-proc.org/10.1051/proc/201965445},
	doi = {10.1051/proc/201965445},
	abstract = {Since the pioneering works of Lapicque [17] and of Hodgkin and Huxley [16], several types of models have been addressed to describe the evolution in time of the potential of the membrane of a neuron. In this note, we investigate a connected version of N neurons obeying the leaky integrate and ﬁre model, previously introduced in [1–3,6,7,15,18,19,22]. As a main feature, neurons interact with one another in a mean ﬁeld instantaneous way. Due to the instantaneity of the interactions, singularities may emerge in a ﬁnite time. For instance, the solution of the corresponding Fokker-Planck equation describing the collective behavior of the potentials of the neurons in the limit N → ∞ may degenerate and cease to exist in any standard sense after a ﬁnite time. Here we focus out on a variant of this model when the interactions between the neurons are also subjected to random synaptic weights. As a typical instance, we address the case when the connection graph is the realization of an Erdo¨s-Renyi graph. After a brief introduction of the model, we collect several theoretical results on the behavior of the solution. In a last step, we provide an algorithm for simulating a network of this type with a possibly large value of N .},
	pages = {445--475},
	journaltitle = {{ESAIM}: Proceedings and Surveys},
	shortjournal = {{ESAIM}: {ProcS}},
	author = {Grazieschi, Paolo and Leocata, Marta and Mascart, Cyrille and Chevallier, Julien and Delarue, François and Tanré, Etienne},
	editor = {Bouchard, B. and Chassagneux, J.-F. and Delarue, F. and Gobet, E. and Lelong, J.},
	urldate = {2024-09-08},
	date = {2019},
	langid = {english},
}

@book{liggett_continuous_2010,
	location = {Providence, R.I},
	title = {Continuous time Markov processes: an introduction},
	isbn = {978-0-8218-4949-1},
	series = {Graduate studies in mathematics},
	shorttitle = {Continuous time Markov processes},
	abstract = {"Markov processes are among the most important stochastic processes for both theory and applications. This book develops the general theory of these processes and applies this theory to various special examples. The initial chapter is devoted to the most important classical example--one-dimensional Brownian motion. This, together with a chapter on continuous time Markov chains, provides the motivation for the general setup based on semigroups and generators. Chapters on stochastic calculus and probabilistic potential theory give an introduction to some of the key areas of application of Brownian motion and its relatives. A chapter on interacting particle systems treats a more recently developed class of Markov processes that have as their origin problems in physics and biology."--Publisher's description},
	pagetotal = {271},
	number = {v. 113},
	publisher = {American Mathematical Society},
	author = {Liggett, Thomas M.},
	date = {2010},
	langid = {english},
	note = {{OCLC}: ocn468231047},
	keywords = {Markov processes, Stochastic integrals},
}

@artwork{wikipedia_neuron-no_2009,
	title = {Neuron-no labels2.png in Inkscape and hand-tuned to reduce filesize},
	rights = {Permission is granted to copy, distribute and/or modify this document under the terms of the {GNU} Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled {GNU} Free Documentation License.http://www.gnu.org/copyleft/fdl.{htmlGFDLGNU} Free Documentation License},
	url = {https://commons.wikimedia.org/wiki/File:Neuron_Hand-tuned.svg},
	author = {Wikipedia, Quasar Jarosz at English},
	urldate = {2023-06-15},
	date = {2009-08-11},
	note = {{URL}: https://commons.wikimedia.org/wiki/File:Neuron\_Hand-tuned.svg},
}

@book{sideris_ordinary_2013,
	location = {Paris},
	title = {Ordinary differential equations and dynamical systems},
	isbn = {978-94-6239-021-8},
	url = {https://link.springer.com/book/10.2991/978-94-6239-021-8},
	series = {Atlantis Studies in Differential Equations},
	pagetotal = {225},
	number = {2214-6253},
	publisher = {Atlantis Press},
	author = {Sideris, Thomas C.},
	urldate = {2023-07-09},
	date = {2013},
	langid = {english},
	doi = {10.2991/978-94-6239-021-8},
}

@incollection{agarwal_banach_2018,
	location = {Singapore},
	edition = {1},
	title = {Banach Contraction Principle and Applications},
	isbn = {978-9-811-32913-5},
	url = {https://link.springer.com/chapter/10.1007/978-981-13-2913-5_1},
	abstract = {Banach contraction principle is a fundamental result in Metric Fixed Point Theory. It is a very popular and powerful tool in solving the existence problems in pure and applied sciences. In this chapter, Banach contraction principle and its converse are presented. Moreover, various applications of this famous principle, including mixed Volterra–Fredholm-type integral equations and systems of nonlinear matrix equations, are provided. Some results of this chapter appeared in [3, 5, 13, 19].},
	pages = {1--23},
	booktitle = {Fixed Point Theory in Metric Spaces: Recent Advances and Applications},
	publisher = {Springer},
	author = {Agarwal, Praveen and Jleli, Mohamed and Samet, Bessem},
	editor = {Agarwal, Praveen and Jleli, Mohamed and Samet, Bessem},
	urldate = {2023-07-09},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-981-13-2913-5_1},
	keywords = {Banach Contraction Principle, Famous Principle, Fixed Point Theory, Nonlinear Matrix Equation, Nonlinear Parabolic Boundary Value Problems},
}

@article{brette_adaptive_2005,
	title = {Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity},
	volume = {94},
	issn = {1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.00686.2005},
	doi = {10.1152/jn.00686.2005},
	abstract = {We introduce a two-dimensional integrate-and-fire model that combines an exponential spike mechanism with an adaptation equation, based on recent theoretical findings. We describe a systematic method to estimate its parameters with simple electrophysiological protocols (current-clamp injection of pulses and ramps) and apply it to a detailed conductance-based model of a regular spiking neuron. Our simple model predicts correctly the timing of 96\% of the spikes (±2 ms) of the detailed model in response to injection of noisy synaptic conductances. The model is especially reliable in high-conductance states, typical of cortical activity in vivo, in which intrinsic conductances were found to have a reduced role in shaping spike trains. These results are promising because this simple model has enough expressive power to reproduce qualitatively several electrophysiological classes described in vitro.},
	pages = {3637--3642},
	number = {5},
	journaltitle = {Journal of Neurophysiology},
	shortjournal = {Journal of Neurophysiology},
	author = {Brette, Romain and Gerstner, Wulfram},
	urldate = {2023-06-07},
	date = {2005-11},
	langid = {english},
}

@article{markram_regulation_1997,
	title = {Regulation of synaptic efficacy by coincidence of postsynaptic {APs} and {EPSPs}},
	volume = {275},
	issn = {0036-8075},
	url = {https://www.science.org/doi/10.1126/science.275.5297.213},
	doi = {10.1126/science.275.5297.213},
	abstract = {Activity-driven modifications in synaptic connections between neurons in the neocortex may occur during development and learning. In dual whole-cell voltage recordings from pyramidal neurons, the coincidence of postsynaptic action potentials ({APs}) and unitary excitatory postsynaptic potentials ({EPSPs}) was found to induce changes in {EPSPs}. Their average amplitudes were differentially up- or down-regulated, depending on the precise timing of postsynaptic {APs} relative to {EPSPs}. These observations suggest that {APs} propagating back into dendrites serve to modify single active synaptic connections, depending on the pattern of electrical activity in the pre- and postsynaptic neurons.},
	pages = {213--215},
	number = {5297},
	journaltitle = {Science (New York, N.Y.)},
	shortjournal = {Science},
	author = {Markram, H. and Lübke, J. and Frotscher, M. and Sakmann, B.},
	urldate = {2023-07-09},
	date = {1997-01-10},
	pmid = {8985014},
	keywords = {Action Potentials, Animals, Calcium, Cerebral Cortex, Dendrites, Down-Regulation, Electric Stimulation, In Vitro Techniques, Patch-Clamp Techniques, Pyramidal Cells, Rats, Rats, Wistar, Receptors, N-Methyl-D-Aspartate, Synapses, Synaptic Transmission, Time Factors, Up-Regulation},
}

@book{gerstner_neuronal_2014,
	location = {Cambridge, United Kingdom},
	edition = {1},
	title = {Neuronal dynamics: from single neurons to networks and models of cognition},
	isbn = {978-1-107-44761-5},
	url = {https://www.cambridge.org/core/books/neuronal-dynamics/75375090046733765596191E23B2959D},
	shorttitle = {Neuronal dynamics},
	abstract = {What happens in our brain when we make a decision? What triggers a neuron to send out a signal? What is the neural code? This textbook for advanced undergraduate and beginning graduate students provides a thorough and up-to-date introduction to the fields of computational and theoretical neuroscience. It covers classical topics, including the Hodgkin-Huxley equations and Hopfield model, as well as modern developments in the field such as Generalized Linear Models and decision theory. Concepts are introduced using clear step-by-step explanations suitable for readers with only a basic knowledge of differential equations and probabilities, and are richly illustrated by figures and worked-out examples. End-of-chapter summaries and classroom-tested exercises make the book ideal for courses or for self-study. The authors also give pointers to the literature and an extensive bibliography, which will prove invaluable to readers interested in further study.},
	pagetotal = {577},
	publisher = {Cambridge University Press},
	author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
	urldate = {2023-07-09},
	date = {2014},
	langid = {english},
	doi = {10.1017/CBO9781107447615},
	keywords = {Cognitive neuroscience, Neural networks (Neurobiology), Neurobiology},
}

@book{bogachev_measure_2007,
	location = {Berlin, Heidelberg},
	title = {Measure Theory},
	isbn = {978-3-540-34514-5},
	url = {http://link.springer.com/10.1007/978-3-540-34514-5},
	pagetotal = {1101},
	publisher = {Springer},
	author = {Bogachev, Vladimir I.},
	urldate = {2023-07-09},
	date = {2007},
	langid = {english},
	doi = {10.1007/978-3-540-34514-5},
	keywords = {Derivative, Lebesgue integral, Measure theory, Transformation, convergence of measures, differential equation, linear optimization, measure, transformation of mesures},
}

@book{hebb_organization_2002,
	location = {New York},
	edition = {1},
	title = {The Organization of Behavior: A Neuropsychological Theory},
	isbn = {978-1-4106-1240-3},
	url = {https://www.taylorfrancis.com/books/mono/10.4324/9781410612403/organization-behavior-hebb},
	shorttitle = {The Organization of Behavior},
	abstract = {Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists--the inability to obtain one of the most cited publications in the field.   The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks.   D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development.   References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology--a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.},
	pagetotal = {378},
	publisher = {Psychology Press},
	author = {Hebb, D. O.},
	urldate = {2023-07-09},
	date = {2002-05-01},
	langid = {english},
	doi = {10.4324/9781410612403},
}

@book{dembo_large_2010,
	location = {Berlin, Heidelberg},
	title = {Large Deviations Techniques and Applications},
	volume = {38},
	isbn = {978-3-642-03311-7},
	url = {http://link.springer.com/10.1007/978-3-642-03311-7},
	series = {Stochastic Modelling and Applied Probability},
	publisher = {Springer Berlin Heidelberg},
	author = {Dembo, Amir and Zeitouni, Ofer},
	urldate = {2023-06-29},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-3-642-03311-7},
}

@incollection{mosier_chapter_2010,
	location = {Philadelphia},
	edition = {5},
	title = {{CHAPTER} 1 - Clinical Neuroscience},
	isbn = {978-0-323-05712-7},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323057127000015},
	pages = {7--17},
	booktitle = {Neurology Secrets},
	publisher = {Mosby},
	author = {Mosier, Dennis R.},
	editor = {Rolak, Loren A.},
	urldate = {2023-06-30},
	date = {2010-01-01},
	langid = {english},
	doi = {10.1016/B978-0-323-05712-7.00001-5},
}

@book{adams_calculus_2017,
	location = {Toronto},
	edition = {Ninth edition},
	title = {Calculus: a complete course},
	isbn = {978-0-13-415436-7},
	shorttitle = {Calculus},
	pagetotal = {1085},
	publisher = {Pearson Canada Inc.},
	author = {Adams, Robert A. and Essex, Christopher},
	date = {2017},
	langid = {english},
}

@article{fourcaud-trocme_how_2003,
	title = {How Spike Generation Mechanisms Determine the Neuronal Response to Fluctuating Inputs},
	volume = {23},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.23-37-11628.2003},
	doi = {10.1523/JNEUROSCI.23-37-11628.2003},
	pages = {11628--11640},
	number = {37},
	journaltitle = {The Journal of Neuroscience},
	shortjournal = {J. Neurosci.},
	author = {Fourcaud-Trocmé, Nicolas and Hansel, David and Van Vreeswijk, Carl and Brunel, Nicolas},
	urldate = {2023-06-07},
	date = {2003-12-17},
	langid = {english},
}

@article{peletier_jump_2022,
	title = {Jump processes as generalized gradient flows},
	volume = {61},
	issn = {0944-2669, 1432-0835},
	url = {https://link.springer.com/10.1007/s00526-021-02130-2},
	doi = {10.1007/s00526-021-02130-2},
	abstract = {We have created a functional framework for a class of non-metric gradient systems. The state space is a space of nonnegative measures, and the class of systems includes the Forward Kolmogorov equations for the laws of Markov jump processes on Polish spaces. This framework comprises a deﬁnition of a notion of solutions, a method to prove existence, and an archetype uniqueness result. We do this by using only the structure that is provided directly by the dissipation functional, which need not be homogeneous, and we do not appeal to any metric structure.},
	pages = {33},
	number = {1},
	journaltitle = {Calculus of Variations and Partial Differential Equations},
	shortjournal = {Calc. Var.},
	author = {Peletier, Mark A. and Rossi, Riccarda and Savaré, Giuseppe and Tse, Oliver},
	urldate = {2023-06-02},
	date = {2022-02},
	langid = {english},
}

@book{amann_ordinary_1990,
	location = {Berlin - New York},
	edition = {Reprint 2010},
	title = {Ordinary Differential Equations: An Introduction to Nonlinear Analysis},
	isbn = {978-3-11-011515-4},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110853698/html},
	series = {De Guyter studies in mathematics},
	shorttitle = {Ordinary Differential Equations},
	abstract = {Ordinary Differential Equations by Herbert Amann was published on April 20, 2011 by De Gruyter.},
	pagetotal = {467},
	number = {13},
	publisher = {De Gruyter},
	author = {Amann, Herbert},
	translator = {Metzen, Gerhard},
	urldate = {2023-02-25},
	date = {1990-08-01},
	langid = {english},
	doi = {10.1515/9783110853698},
	note = {Publication Title: Ordinary Differential Equations},
	keywords = {Gewöhnliche Differentialgleichung},
}

@book{wulfram_gerstner_spiking_2002,
	location = {Cambridge, U.K.},
	title = {Spiking Neuron Models : Single Neurons, Populations, Plasticity},
	isbn = {978-0-521-81384-6},
	url = {https://doi.org/10.1017/CBO9780511815706},
	shorttitle = {Spiking Neuron Models},
	abstract = {Neurons in the brain communicate by short electrical pulses, the so-called action potentials or spikes. How can we understand the process of spike generation? How can we understand information transmission by neurons? What happens if thousands of neurons are coupled together in a seemingly random network? How does the network connectivity determine the activity patterns? And, vice versa, how does the spike activity influence the connectivity pattern? These questions are addressed in this 2002 introduction to spiking neurons aimed at those taking courses in computational neuroscience, theoretical biology, biophysics, or neural networks. The approach will suit students of physics, mathematics, or computer science; it will also be useful for biologists who are interested in mathematical modelling. The text is enhanced by many worked examples and illustrations. There are no mathematical prerequisites beyond what the audience would meet as undergraduates: more advanced techniques are introduced in an elementary, concrete fashion when needed.},
	publisher = {Cambridge University Press},
	author = {{Wulfram Gerstner} and {Werner M. Kistler}},
	urldate = {2023-04-20},
	date = {2002},
	keywords = {{COMPUTERS} / Data Science / Neural Networks, Computational neuroscience, Neural circuitry, Neural networks (Neurobiology), Neurons, Neuroplasticity, {SCIENCE} / Life Sciences / Neuroscience},
}

@article{rackauckas_differentialequationsjl_2017,
	title = {{DifferentialEquations}.jl – A Performant and Feature-Rich Ecosystem for Solving Differential Equations in Julia},
	volume = {5},
	issn = {2049-9647},
	url = {https://openresearchsoftware.metajnl.com/article/10.5334/jors.151/},
	doi = {10.5334/jors.151},
	pages = {15},
	number = {1},
	journaltitle = {Journal of Open Research Software},
	shortjournal = {{JORS}},
	author = {Rackauckas, Christopher and Nie, Qing},
	urldate = {2023-04-13},
	date = {2017-05-25},
	langid = {english},
}

@article{davis_piecewise-deterministic_1984,
	title = {Piecewise-Deterministic Markov Processes: A General Class of Non-Diffusion Stochastic Models},
	volume = {46},
	issn = {2517-6161},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1984.tb01308.x},
	doi = {10.1111/j.2517-6161.1984.tb01308.x},
	shorttitle = {Piecewise-Deterministic Markov Processes},
	abstract = {A general class of non-diffusion stochastic models is introduced with a view to providing a framework for studying optimization problems arising in queueing systems, inventory theory, resource allocation and other areas. The corresponding stochastic processes are Markov processes consisting of a mixture of deterministic motion and random jumps. Stochastic calculus for these processes is developed and a complete characterization of the extended generator is given; this is the main technical result of the paper. The relevance of the extended generator concept in applied problems is discussed and some recent results on optimal control of piecewise-deterministic processes are described.},
	pages = {353--376},
	number = {3},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
	shortjournal = {J R Stat Soc Series B Stat Methodol},
	author = {Davis, M. H. A.},
	urldate = {2023-03-29},
	date = {1984},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1984.tb01308.x},
	keywords = {capacity expansion, dynamic programming, extended generator, markov process, martingale, queueing theory, stochastic control theory, stochastic models},
}

@incollection{hormander_test_1998,
	location = {Berlin, Heidelberg},
	title = {Test Functions},
	isbn = {978-3-642-96750-4},
	url = {https://doi.org/10.1007/978-3-642-96750-4_2},
	series = {Grundlehren der mathematischen Wissenschaften},
	shorttitle = {The Analysis of Linear Partial Differential Operators I},
	abstract = {As indicated in the introduction one must work consistently with smooth “test functions” in the theory of distributions. In this chapter we have collected the basic facts that one needs to know about such functions. As an introduction a brief summary of differential calculus is given in Section 1.1. It is written with a reader in mind who has seen the material before but perhaps with different emphasis and different notation. The reader who finds the presentation hard to follow is recommended to study first a more thorough modern treatment of differential calculus in several variables, and experienced readers should proceed directly to Section 1.2. In addition to the basic indispensible facts we have included in Sections 1.3 and 1.4 some more refined constructions which will be useful some time in this book but are not important for the main theme. The reader in a hurry may therefore wish to omit Section 1.3 from Theorem 1.3.5 on and also Theorem 1.4.2, Lemma 1.4.3 and the rest of Section 1.4 from Theorem 1.4.6 on.},
	pages = {5--32},
	booktitle = {The Analysis of Linear Partial Differential Operators I: Distribution Theory and Fourier Analysis},
	publisher = {Springer},
	author = {Hörmander, Lars},
	editor = {Hörmander, Lars},
	urldate = {2023-03-16},
	date = {1998},
	langid = {english},
	doi = {10.1007/978-3-642-96750-4_2},
	keywords = {Banach Space, Cutoff Function, Differential Calculus, Inverse Function Theorem, Multilinear Form},
}

@incollection{hormander_definition_1998,
	location = {Berlin, Heidelberg},
	title = {Definition and Basic Properties of Distributions},
	isbn = {978-3-642-96750-4},
	url = {https://doi.org/10.1007/978-3-642-96750-4_3},
	series = {Grundlehren der mathematischen Wissenschaften},
	shorttitle = {The Analysis of Linear Partial Differential Operators I},
	abstract = {In the introduction we have seen how various difficulties in the theory of partial differential equations and in Fourier analysis lead one to extend the space of continuous functions to the space of distributions. In Section 2.1 we make the definition explicit and precise, using the properties of test functions proved in Chapter I. The weak topology in the space of distributions is also introduced there. The notion of support is extended to distributions in Section 2.2 and it is shown there that distributions may be defined locally provided that the local definitions are compatible. In addition it is proved that if u is a distribution then there is a unique way to define u(ϕ) for all ϕ∈C∞ with supp u ∩ supp ϕ compact. The problem of estimating u(ϕ) in terms of the derivatives of ϕ on the support of u only is discussed at some length in Section 2.3. The deepest result is Whitney’s extension theorem (Theorem 2.3.6). We shall rarely need the results which follow from it so the reader might prefer to skip the section from Theorem 2.3.6 on.},
	pages = {33--53},
	booktitle = {The Analysis of Linear Partial Differential Operators I: Distribution Theory and Fourier Analysis},
	publisher = {Springer},
	author = {Hörmander, Lars},
	editor = {Hörmander, Lars},
	urldate = {2023-03-16},
	date = {1998},
	langid = {english},
	doi = {10.1007/978-3-642-96750-4_3},
}

@incollection{golse_dynamics_2016,
	edition = {1},
	title = {On the Dynamics of Large Particle Systems in the Mean Field Limit},
	volume = {3},
	isbn = {978-3-319-26883-5},
	url = {http://arxiv.org/abs/1301.5494},
	series = {Lecture Notes in Applied Mathematics and Mechanics},
	abstract = {This course explains how the usual mean field evolution partial differential equations ({PDEs}) in Statistical Physics - such as the Vlasov-Poisson system, the vorticity formulation of the two-dimensional Euler equation for incompressible fluids, or the time-dependent Hartree equation in quantum mechanics - can be rigorously derived from first principles, i.e. from the fundamental microscopic equations that govern the evolution of large, interacting particle systems. The emphasis is put on the mathematical methods used in these derivations, such as Dobrushin's stability estimate in the Monge-Kantorovich distance for the empirical measures built on the solution of the N-particle motion equations in classical mechanics, or the theory of {BBGKY} hierarchies in the case of classical as well as quantum problems. We explain in detail how these different approaches are related; in particular we insist on the notion of chaotic sequences and on the propagation of chaos in the {BBGKY} hierarchy as the number of particles tends to infinity.},
	pages = {1--144},
	booktitle = {Macroscopic and Large Scale Phenomena: Coarse Graining, Mean Field Limits and Ergodicity},
	publisher = {Springer Cham},
	author = {Golse, François},
	urldate = {2023-03-16},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-26883-5_1},
	eprinttype = {arxiv},
	eprint = {1301.5494 [math-ph]},
	keywords = {82C22 (Primary) 35A10, 35Q40, 82C05, 82C10 (Secondary), Mathematical Physics, Mathematics - Analysis of {PDEs}},
}

@article{parr_modules_2020,
	title = {Modules or Mean-Fields?},
	volume = {22},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/22/5/552},
	doi = {10.3390/e22050552},
	abstract = {The segregation of neural processing into distinct streams has been interpreted by some as evidence in favour of a modular view of brain function. This implies a set of specialised ‘modules’, each of which performs a specific kind of computation in isolation of other brain systems, before sharing the result of this operation with other modules. In light of a modern understanding of stochastic non-equilibrium systems, like the brain, a simpler and more parsimonious explanation presents itself. Formulating the evolution of a non-equilibrium steady state system in terms of its density dynamics reveals that such systems appear on average to perform a gradient ascent on their steady state density. If this steady state implies a sufficiently sparse conditional independency structure, this endorses a mean-field dynamical formulation. This decomposes the density over all states in a system into the product of marginal probabilities for those states. This factorisation lends the system a modular appearance, in the sense that we can interpret the dynamics of each factor independently. However, the argument here is that it is factorisation, as opposed to modularisation, that gives rise to the functional anatomy of the brain or, indeed, any sentient system. In the following, we briefly overview mean-field theory and its applications to stochastic dynamical systems. We then unpack the consequences of this factorisation through simple numerical simulations and highlight the implications for neuronal message passing and the computational architecture of sentience.},
	pages = {552},
	number = {5},
	journaltitle = {Entropy},
	shortjournal = {Entropy},
	author = {Parr, Thomas and Sajid, Noor and Friston, Karl J.},
	urldate = {2023-03-13},
	date = {2020-05-14},
	langid = {english},
}

@article{caceres_analysis_2011,
	title = {Analysis of Nonlinear Noisy Integrate \& Fire Neuron Models: blow-up and steady states},
	volume = {1},
	issn = {2190-8567},
	url = {http://mathematical-neuroscience.springeropen.com/articles/10.1186/2190-8567-1-7},
	doi = {10.1186/2190-8567-1-7},
	shorttitle = {Analysis of Nonlinear Noisy Integrate\&Fire Neuron Models},
	abstract = {Nonlinear Noisy Leaky Integrate and Fire ({NNLIF}) models for neurons networks can be written as Fokker-Planck-Kolmogorov equations on the probability density of neurons, the main parameters in the model being the connectivity of the network and the noise. We analyse several aspects of the {NNLIF} model: the number of steady states, a priori estimates, blow-up issues and convergence toward equilibrium in the linear case. In particular, for excitatory networks, blow-up always occurs for initial data concentrated close to the ﬁring potential. These results show how critical is the balance between noise and excitatory/inhibitory interactions to the connectivity parameter.},
	pages = {33},
	number = {7},
	journaltitle = {The Journal of Mathematical Neuroscience},
	shortjournal = {The Journal of Mathematical Neuroscience},
	author = {Cáceres, María J and Carrillo, José A and Perthame, Benoît},
	urldate = {2023-02-27},
	date = {2011},
	langid = {english},
}

@article{prokert_lecture_nodate,
	title = {Lecture Notes Ordinary Diﬀerential Equations (2WA70)},
	author = {Prokert, G},
	langid = {english},
}

@collection{goncalves_particle_2017,
	location = {Cham},
	title = {From Particle Systems to Partial Differential Equations: {PSPDE} {IV}, Braga, Portugal, December 2015},
	volume = {209},
	isbn = {978-3-319-66838-3 978-3-319-66839-0},
	url = {http://link.springer.com/10.1007/978-3-319-66839-0},
	series = {Springer Proceedings in Mathematics \& Statistics},
	shorttitle = {From Particle Systems to Partial Differential Equations},
	publisher = {Springer International Publishing},
	editor = {Gonçalves, Patrícia and Soares, Ana Jacinta},
	urldate = {2023-02-17},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-3-319-66839-0},
}

@article{reed_generalist_2022,
	title = {A Generalist Agent},
	volume = {10},
	issn = {2835-8856},
	url = {http://arxiv.org/abs/2205.06175},
	doi = {10.48550/arXiv.2205.06175},
	abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
	pages = {9--51},
	number = {6},
	journaltitle = {Transactions on Machine Learning Research},
	shortjournal = {{TMLR}},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
	urldate = {2023-02-17},
	date = {2022-11-11},
	eprinttype = {arxiv},
	eprint = {2205.06175 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{treves_mean-field_1992,
	title = {Mean-field analysis of neuronal spike dynamics},
	volume = {4},
	rights = {© 1993 Informa {UK} Ltd All rights reserved: reproduction in whole or part not permitted},
	url = {https://www.tandfonline.com/doi/abs/10.1088/0954-898X_4_3_002},
	doi = {10.1088/0954-898X_4_3_002},
	abstract = {I consider a mean-field description of the dynamics of interacting intergrate-and-fire neuron-like units. The basic dynamical variables are the membrane potential of each (point-like) ‘cell’ and th...},
	pages = {259--284},
	number = {3},
	journaltitle = {Network: Computation in Neural Systems},
	author = {Treves, Alessandro},
	urldate = {2023-02-17},
	date = {1992-10-05},
	langid = {english},
	note = {Publisher: Taylor \& Francis},
}

@article{brunel_fast_1999,
	title = {Fast Global Oscillations in Networks of Integrate-and-Fire Neurons with Low Firing Rates},
	volume = {11},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/089976699300016179},
	doi = {10.1162/089976699300016179},
	abstract = {We study analytically the dynamics of a network of sparsely connected inhibitory integrate-and-fire neurons in a regime where individual neurons emit spikes irregularly and at a low rate. In the limit when the number of neurons N → ∞, the network exhibits a sharp transition between a stationary and an oscillatory global activity regime where neurons are weakly synchronized. The activity becomes oscillatory when the inhibitory feedback is strong enough. The period of the global oscillation is found to be mainly controlled by synaptic times but depends also on the characteristics of the external input. In large but finite networks, the analysis shows that global oscillations of finite coherence time generically exist both above and below the critical inhibition threshold. Their characteristics are determined as functions of systems parameters in these two different regimes. The results are found to be in good agreement with numerical simulations.},
	pages = {1621--1671},
	number = {7},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Brunel, Nicolas and Hakim, Vincent},
	urldate = {2023-02-17},
	date = {1999-10-01},
}

@book{spohn_large_1991,
	location = {Berlin, Heidelberg},
	title = {Large Scale Dynamics of Interacting Particles},
	isbn = {978-3-642-84373-0 978-3-642-84371-6},
	url = {http://link.springer.com/10.1007/978-3-642-84371-6},
	publisher = {Springer Berlin Heidelberg},
	author = {Spohn, Herbert},
	urldate = {2023-02-17},
	date = {1991},
	langid = {english},
	doi = {10.1007/978-3-642-84371-6},
}

@incollection{gerstner_exponential_2014,
	location = {Cambridge, United Kingdom},
	edition = {1},
	title = {Exponential integrate-and-fire model},
	isbn = {978-1-107-63519-7 978-1-107-06083-8},
	url = {https://www.cambridge.org/core/books/neuronal-dynamics/75375090046733765596191E23B2959D},
	abstract = {What happens in our brain when we make a decision? What triggers a neuron to send out a signal? What is the neural code? This textbook for advanced undergraduate and beginning graduate students provides a thorough and up-to-date introduction to the fields of computational and theoretical neuroscience. It covers classical topics, including the Hodgkin-Huxley equations and Hopfield model, as well as modern developments in the field such as Generalized Linear Models and decision theory. Concepts are introduced using clear step-by-step explanations suitable for readers with only a basic knowledge of differential equations and probabilities, and are richly illustrated by figures and worked-out examples. End-of-chapter summaries and classroom-tested exercises make the book ideal for courses or for self-study. The authors also give pointers to the literature and an extensive bibliography, which will prove invaluable to readers interested in further study.},
	pages = {124--129},
	booktitle = {Neuronal dynamics: from single neurons to networks and models of cognition},
	publisher = {Cambridge University Press},
	author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
	date = {2014},
	langid = {english},
	keywords = {Cognitive neuroscience, Neural networks (Neurobiology), Neurobiology},
}

@article{gerstner_mathematical_2002,
	title = {Mathematical formulations of Hebbian learning},
	volume = {87},
	issn = {03401200},
	url = {http://link.springer.com/10.1007/s00422-002-0353-y},
	doi = {10.1007/s00422-002-0353-y},
	abstract = {Several formulations of correlation-based Hebbian learning are reviewed. On the presynaptic side, activity is described either by a ﬁring rate or by presynaptic spike arrival. The state of the postsynaptic neuron can be described by its membrane potential, its ﬁring rate, or the timing of backpropagating action potentials ({BPAPs}). It is shown that all of the above formulations can be derived from the point of view of an expansion. In the absence of {BPAPs}, it is natural to correlate presynaptic spikes with the postsynaptic membrane potential. Time windows of spike-time-dependent plasticity arise naturally if the timing of postsynaptic spikes is available at the site of the synapse, as is the case in the presence of {BPAPs}. With an appropriate choice of parameters, Hebbian synaptic plasticity has intrinsic normalization properties that stabilizes postsynaptic ﬁring rates and leads to subtractive weight normalization.},
	pages = {404--415},
	number = {5},
	journaltitle = {Biological Cybernetics},
	author = {Gerstner, Wulfram and Kistler, Werner M.},
	urldate = {2023-02-16},
	date = {2002-12-01},
	langid = {english},
}

@article{perthame_distributed_2017,
	title = {Distributed synaptic weights in a {LIF} neural network and learning rules},
	volume = {353-354},
	issn = {01672789},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167278916304791},
	doi = {10.1016/j.physd.2017.05.005},
	abstract = {Leaky integrate-and-fire ({LIF}) models are mean-field limits, with a large number of neurons, used to describe neural networks. We consider inhomogeneous networks structured by a connectivity parameter (strengths of the synaptic weights) with the effect of processing the input current with different intensities.},
	pages = {20--30},
	journaltitle = {Physica D: Nonlinear Phenomena},
	shortjournal = {Physica D: Nonlinear Phenomena},
	author = {Perthame, Benoît and Salort, Delphine and Wainrib, Gilles},
	urldate = {2023-02-16},
	date = {2017-09},
	langid = {english},
}

@misc{tse_modelling_2015,
	title = {Modelling with deterministic and stochastic equations: Micro-, meso- and macroscopic scales},
	author = {Tse, Oliver},
	date = {2015-03-25},
	langid = {english},
}

@misc{tse_fundamentals_2015,
	title = {Fundamentals from Measure Theory},
	author = {Tse, Oliver},
	date = {2015-11-12},
	langid = {english},
}
